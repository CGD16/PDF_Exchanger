{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\" # https://stackoverflow.com/questions/35911252/disable-tensorflow-debugging-information\n",
    "\n",
    "import ast\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "import keras.layers as KL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 512\n",
    "patch_size=16\n",
    "frozen_stages = 0\n",
    "in_chans = 3\n",
    "embed_dim = [224, 336, 448]\n",
    "partial_dim = [48, 72, 96] # partial_dim = r*embed_dim with r=1/4.67\n",
    "qk_dim = [16, 16, 16]\n",
    "depth = [4, 7, 6]\n",
    "types = [\"i\", \"s\", \"s\"]\n",
    "down_ops = [['subsample', 2], ['subsample', 2], ['']]\n",
    "pretrained = None\n",
    "distillation = False\n",
    "\n",
    "train_bn = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupNorm(KL.Layer):\n",
    "    \"\"\"\n",
    "    This implementation assumes the input tensor shape is [B, H, W, C], which is typical in TensorFlow/Keras, as opposed to [B, C, H, W] in PyTorch.\n",
    "    The GroupNorm here normalizes over spatial dimensions (height and width) while keeping the channel dimension intact.\n",
    "    mean and variance are computed across the spatial dimensions.\n",
    "\n",
    "    Args:\n",
    "        num_channels (int): Number of channels in the input tensor.\n",
    "        num_groups (int): Number of groups to separate the channels into. Each group will be normalized independently.\n",
    "        epsilon (float): Small value to add to the denominator for numerical stability.\n",
    "\n",
    "    Returns:\n",
    "        Normalized input tensor with the same shape as the input tensor.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_channels, num_groups=1, **kwargs):\n",
    "        super(GroupNorm, self).__init__(**kwargs)\n",
    "        self.num_channels = num_channels\n",
    "        self.num_groups = num_groups\n",
    "        self.epsilon = 1e-5\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Reshape input to (B, H, W, C)\n",
    "        inputs = tf.convert_to_tensor(inputs)\n",
    "        B, H, W, C = tf.shape(inputs)[0], tf.shape(inputs)[1], tf.shape(inputs)[2], self.num_channels\n",
    "        \n",
    "        # Reshape for group normalization\n",
    "        inputs = tf.reshape(inputs, (B, H, W, self.num_groups, C // self.num_groups))\n",
    "        \n",
    "        # Calculate mean and variance for each group\n",
    "        mean, variance = tf.nn.moments(inputs, axes=[1, 2, 4], keepdims=True)\n",
    "\n",
    "        # Normalize\n",
    "        normalized = (inputs - mean) / tf.sqrt(variance + self.epsilon)\n",
    "\n",
    "        # Reshape back to original dimensions\n",
    "        normalized = tf.reshape(normalized, (B, H, W, C))\n",
    "\n",
    "        return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2d_BN(KL.Layer):\n",
    "    \"\"\"\n",
    "    Conv2D layer with optional BatchNormalization and Activation.\n",
    "    \n",
    "    Args:\n",
    "        a: int, default 0, the number of input channels\n",
    "        filters: int, the number of output channels\n",
    "        kernel_size: int, the size of the kernel\n",
    "        strides: int, the stride of the convolution\n",
    "        padding: str, the padding mode\n",
    "        dilation_rate: int, the dilation rate\n",
    "        groups: int, the number of groups for grouped convolution\n",
    "        use_bn: bool, whether to use BatchNormalization\n",
    "        activation: str, the activation function\n",
    "\n",
    "    Returns:\n",
    "        x: tensor, the output tensor\n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(self, a=0, filters=16, kernel_size=1, strides=1, padding='same', dilation_rate=1, groups=1, use_bn=True, activation=\"relu\"):\n",
    "        super(Conv2d_BN, self).__init__()\n",
    "        self.conv = KL.Conv2D(filters, kernel_size, strides=strides, padding=padding, dilation_rate=1, groups=1, use_bias=not use_bn)\n",
    "        self.bn = KL.BatchNormalization() if use_bn else None\n",
    "        self.activation = KL.Activation(activation) if activation else None\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        x = self.conv(x)\n",
    "        if self.bn:\n",
    "            x = self.bn(x, training=training)\n",
    "        if self.activation:\n",
    "            x = self.activation(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy from https://github.com/huggingface/pytorch-image-models/blob/main/timm/layers/helpers.py\n",
    "def make_divisible(v, divisor=8, min_value=None, round_limit=.9):\n",
    "    min_value = min_value or divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < round_limit * v:\n",
    "        new_v += divisor\n",
    "    return new_v\n",
    "\n",
    "# translated with ChatGPT (original from https://github.com/huggingface/pytorch-image-models/blob/main/timm/layers/squeeze_excite.py)\n",
    "class SqueezeExcite(KL.Layer):\n",
    "    \"\"\"\n",
    "    Referenzen:\n",
    "    - https://arxiv.org/abs/1709.01507\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels, rd_ratio=1/16, rd_channels=None, rd_divisor=8, add_maxpool=False,\n",
    "                 bias=True, act_layer=\"relu\", norm_layer=None, gate_layer=\"sigmoid\"):\n",
    "        super(SqueezeExcite, self).__init__()\n",
    "        self.add_maxpool = add_maxpool\n",
    "        if not rd_channels:\n",
    "            rd_channels = make_divisible(int(channels * rd_ratio), rd_divisor)\n",
    "        \n",
    "        self.fc1 = KL.Conv2D(filters=rd_channels, kernel_size=1, use_bias=bias)\n",
    "        self.bn = norm_layer() if norm_layer else KL.Lambda(lambda x: x)\n",
    "        self.act = KL.Activation(act_layer)\n",
    "        self.fc2 = KL.Conv2D(filters=channels, kernel_size=1, use_bias=bias)\n",
    "        self.gate = KL.Activation(gate_layer)\n",
    "\n",
    "    def call(self, x):\n",
    "        x_se = tf.reduce_mean(x, axis=[1, 2], keepdims=True)\n",
    "        if self.add_maxpool:\n",
    "            x_se = 0.5 * x_se + 0.5 * tf.reduce_max(x, axis=[1, 2], keepdims=True)\n",
    "        x_se = self.fc1(x_se)\n",
    "        x_se = self.act(self.bn(x_se))\n",
    "        x_se = self.fc2(x_se)\n",
    "        return x * self.gate(x_se)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchMerging(KL.Layer):\n",
    "    \"\"\"\n",
    "    Initializes three Conv2d_BN layers and an activation function.\n",
    "    The call method processes the input through these layers sequentially, applying ReLU activations and squeeze-and-excitation before the final convolution.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        out_dim (int): Number of output channels.\n",
    "        train_bn (bool): Whether to use batch normalization.\n",
    "    \n",
    "    Returns:\n",
    "        x (tensor): Output tensor.\n",
    "    \"\"\" \n",
    "    def __init__(self, dim, out_dim, train_bn=True, **kwargs):\n",
    "        super(PatchMerging, self).__init__(**kwargs)\n",
    "        hid_dim = int(dim * 4)\n",
    "        self.conv1 = Conv2d_BN(a=dim, filters=hid_dim, kernel_size=1, use_bn=train_bn)\n",
    "        self.act = keras.activations.relu\n",
    "        self.conv2 = Conv2d_BN(a=hid_dim, filters=hid_dim, kernel_size=3, strides=2, padding=\"same\", groups=hid_dim, use_bn=train_bn)\n",
    "        self.se = SqueezeExcite(channels=hid_dim, rd_ratio=0.25)\n",
    "        self.conv3 = Conv2d_BN(a=hid_dim, filters=out_dim, kernel_size=1, use_bn=train_bn)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # print(\"### PatchMerging ###\")\n",
    "        # print(inputs.shape)\n",
    "    \n",
    "        x = self.conv1(inputs)\n",
    "        x = self.act(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.act(x)\n",
    "        x = self.se(x)\n",
    "        x = self.conv3(x)\n",
    "        # print(x.shape)\n",
    "        # print(\"===\"*10)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(KL.Layer):\n",
    "    \"\"\"\n",
    "    Initialization: Takes a layer m and a dropout probability drop.\n",
    "\n",
    "    Forward Pass:\n",
    "        During training, if dropout is active, a random mask is applied to the output of m.\n",
    "        If not in training mode, it simply adds the output of m to the input.\n",
    "\n",
    "    Fusion:\n",
    "        If m is a Conv2d_BN instance, it fuses the convolution and batch normalization layers.\n",
    "        An identity tensor is created, padded, and added to the convolution weights.\n",
    "\n",
    "    Args:\n",
    "        m: a layer instance\n",
    "        drop: dropout probability\n",
    "    \n",
    "    Returns:\n",
    "        The output tensor of the layer\n",
    "    \"\"\"\n",
    "    def __init__(self, m, drop=0.0, **kwargs):\n",
    "        super(Residual, self).__init__(**kwargs)\n",
    "        self.m = m\n",
    "        self.drop = drop\n",
    "\n",
    "    def call(self, inputs, use_bn=None):\n",
    "        if use_bn and self.drop > 0:\n",
    "            # Generate a random mask for dropout\n",
    "            rand_tensor = tf.random.uniform((tf.shape(inputs)[0], 1, 1, 1), 0, 1)\n",
    "            mask = tf.cast(rand_tensor >= self.drop, tf.float32) / (1 - self.drop)\n",
    "            return inputs + self.m(inputs) * mask\n",
    "        else:\n",
    "            return inputs + self.m(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Initialization: Initializes two Conv2d_BN layers (pointwise convolutions) and a ReLU activation.\n",
    "    Forward Pass: Applies the first convolution, then the ReLU activation, and finally the second convolution, returning the output.\n",
    "\n",
    "    Args:\n",
    "        ed (int): Embedding dimension\n",
    "        h (int): Hidden dimension\n",
    "        train_bn (bool): Whether to use batch normalization during training\n",
    "\n",
    "    Returns:\n",
    "        The output tensor of the layer\n",
    "    \"\"\"\n",
    "    def __init__(self, ed, h, train_bn=True, **kwargs):\n",
    "        super(FFN, self).__init__(**kwargs)\n",
    "        self.pw1 = Conv2d_BN(a=ed, filters=h, use_bn=train_bn)  # First pointwise convolution with BN\n",
    "        self.act = keras.activations.relu  # ReLU activation\n",
    "        self.pw2 = Conv2d_BN(a=h, filters=ed, use_bn=train_bn)  # Second pointwise convolution with BN\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # print(\"### FFN ###\")\n",
    "        # print(inputs.shape)\n",
    "        x = self.pw1(inputs)\n",
    "        x = self.act(x)\n",
    "        x = self.pw2(x)\n",
    "        # print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SHSA(KL.Layer):\n",
    "    \"\"\"Single-Head Self-Attention\"\"\"\n",
    "    \"\"\"\n",
    "    Initialization: \n",
    "        Iitializes scaling factor, dimensions, normalization layer, and the query-key-value (QKV) convolutional layer, along with a projection layer.\n",
    "\n",
    "    Forward Pass:\n",
    "        Splits the input into two parts, applies normalization, and computes QKV.\n",
    "        Flattens Q, K, and V, calculates the attention scores, applies softmax, and reshapes the result.\n",
    "        Concatenates the processed part with the other part and passes it through the projection layer.\n",
    "    \n",
    "    Args:\n",
    "        dim (int): Number of input channels\n",
    "        qk_dim (int): Query and key dimension\n",
    "        pdim (int): Partial dimension\n",
    "        train_bn (bool): Whether to use batch normalization during training\n",
    "    \n",
    "    Returns:\n",
    "        The output tensor of the layer\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, qk_dim, pdim, train_bn=True, **kwargs):\n",
    "        super(SHSA, self).__init__(**kwargs)\n",
    "\n",
    "        self.scale = qk_dim ** -0.5\n",
    "        self.qk_dim = qk_dim\n",
    "        self.dim = dim\n",
    "        self.pdim = pdim\n",
    "\n",
    "        self.pre_norm = GroupNorm(num_channels=pdim)  # Assuming GroupNorm is defined\n",
    "        self.qkv = Conv2d_BN(a=pdim, filters=qk_dim * 2 + pdim, use_bn=train_bn)  # Conv2d_BN layer\n",
    "        self.proj = tf.keras.Sequential([\n",
    "            keras.layers.ReLU(),\n",
    "            Conv2d_BN(a=dim, filters=dim, use_bn=train_bn)  # Another Conv2d_BN layer\n",
    "        ])\n",
    "\n",
    "    def call(self, x):\n",
    "        B, H, W, C = tf.shape(x)[0], tf.shape(x)[1], tf.shape(x)[2], tf.shape(x)[3]\n",
    "        x1, x2 = tf.split(x, [self.pdim, self.dim - self.pdim], axis=-1)\n",
    "        x1 = self.pre_norm(x1)\n",
    "        \n",
    "        qkv = self.qkv(x1)\n",
    "        q, k, v = tf.split(qkv, [self.qk_dim, self.qk_dim, self.pdim], axis=-1)\n",
    "        \n",
    "        q = tf.reshape(q, (B, -1, self.qk_dim))\n",
    "        k = tf.reshape(k, (B, -1, self.qk_dim))\n",
    "        v = tf.reshape(v, (B, -1, self.pdim))\n",
    "\n",
    "        attn = tf.matmul(q, k, transpose_b=True) * self.scale\n",
    "        attn = tf.nn.softmax(attn, axis=-1)\n",
    "\n",
    "        x1 = tf.reshape(tf.matmul(attn, v), (B, H, W, self.pdim))\n",
    "        x = self.proj(tf.concat([x1, x2], axis=-1))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(KL.Layer):\n",
    "    \"\"\"\n",
    "    Initialization:\n",
    "        For \"s\" (later stages): Initializes convolution, self-attention mixer, and feed-forward network (\n",
    "        \n",
    "        ) wrapped in residuals.\n",
    "        For \"i\" (early stages): Initializes convolution and FFN as before but uses an identity layer for the mixer.\n",
    "        \n",
    "    Forward Pass:\n",
    "        Calls the convolution layer, the mixer, and the feed-forward network sequentially, returning the output.\n",
    "    \n",
    "    Args:\n",
    "        dim (int): Number of input channels\n",
    "        qk_dim (int): Query and key dimension\n",
    "        pdim (int): Partial dimension\n",
    "        block_type (str): \"s\" for later stages and \"i\" for early stages\n",
    "        train_bn (bool): Whether to use batch normalization during training\n",
    "\n",
    "    Returns:\n",
    "        The output tensor of the layer\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, qk_dim, pdim, block_type, train_bn=True, **kwargs):\n",
    "        super(BasicBlock, self).__init__(**kwargs)\n",
    "        if block_type == \"s\":  # for later stages\n",
    "            self.conv = Residual(Conv2d_BN(a=dim, filters=dim, kernel_size=3, strides=1, padding=\"same\", groups=dim, use_bn=train_bn))\n",
    "            self.mixer = Residual(SHSA(dim=dim, qk_dim=qk_dim, pdim=pdim, train_bn=train_bn))\n",
    "            self.ffn = Residual(FFN(ed=dim, h=int(dim * 2), train_bn=train_bn))\n",
    "        elif block_type == \"i\":  # for early stages\n",
    "            self.conv = Residual(Conv2d_BN(a=dim, filters=dim, kernel_size=3, strides=1, padding=\"same\", groups=dim, use_bn=train_bn))\n",
    "            self.mixer = KL.Layer()  # Identity layer\n",
    "            self.ffn = Residual(FFN(ed=dim, h=int(dim * 2), train_bn=train_bn))\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.ffn(self.mixer(self.conv(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed_dim shape:  [224, 336, 448]\n",
      "Shape of an input image:  (1, 512, 512, 3)\n",
      "First 3x3 strided conv iamge shape:  (1, 256, 256, 28)\n",
      "Second 3x3 strided conv iamge shape:  (1, 128, 128, 56)\n",
      "Third 3x3 strided conv iamge shape:  (1, 64, 64, 112)\n",
      "Fourth 3x3 strided conv shape:  (1, 32, 32, 224)\n"
     ]
    }
   ],
   "source": [
    "# input image [batch, height, width, channels]\n",
    "input_image = tf.random.normal([1, 512, 512, 3])\n",
    "\n",
    "blocks1 = []\n",
    "blocks2 = []\n",
    "blocks3 = []\n",
    "outs = []\n",
    "\n",
    "print(\"embed_dim shape: \", embed_dim)\n",
    "print(\"Shape of an input image: \", input_image.shape)\n",
    "# 16x16 Overlap PatchEmbed (Fig 2 and Fig 5), in SHViT-pytorch version of the code, each Conv2D is followed with BatchNormalization \n",
    "x = Conv2d_BN(filters=embed_dim[0]//8, kernel_size=3, strides=2, padding=\"same\", use_bn=train_bn, activation=\"ReLU\")(input_image)\n",
    "print(\"First 3x3 strided conv iamge shape: \", x.shape)\n",
    "\n",
    "x = Conv2d_BN(filters=embed_dim[0]//4, kernel_size=3, strides=2, padding=\"same\", use_bn=train_bn, activation=\"ReLU\")(x)\n",
    "print(\"Second 3x3 strided conv iamge shape: \", x.shape)\n",
    "\n",
    "x = Conv2d_BN(filters=embed_dim[0]//2, kernel_size=3, strides=2, padding=\"same\", use_bn=train_bn, activation=\"ReLU\")(x)\n",
    "print(\"Third 3x3 strided conv iamge shape: \", x.shape)\n",
    "\n",
    "outs.append(x)\n",
    "outs.append(x) # for C5\n",
    "x = Conv2d_BN(filters=embed_dim[0]//1, kernel_size=3, strides=2, padding=\"same\", use_bn=train_bn, activation=\"ReLU\")(x)\n",
    "\n",
    "# [1, 32, 32, 224] represents the spatial grid of 16x16 patches (32 patches along each dimension) with each patch transformed into a 224-dimensional vector)\n",
    "print(\"Fourth 3x3 strided conv shape: \", x.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 7, 6]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(224, 16, 48, 4, ['subsample', 2], 'i'),\n",
       " (336, 16, 72, 7, ['subsample', 2], 's'),\n",
       " (448, 16, 96, 6, [''], 's')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(depth)\n",
    "list(zip(embed_dim, qk_dim, partial_dim, depth, down_ops, types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 224 16 48 4 ['subsample', 2] i\n",
      "dpth:  4\n",
      "1 336 16 72 7 ['subsample', 2] s\n",
      "dpth:  7\n",
      "2 448 16 96 6 [''] s\n",
      "dpth:  6\n",
      "[<__main__.BasicBlock object at 0x7fcf88bff970>, <__main__.BasicBlock object at 0x7fd0fdf691b0>, <__main__.BasicBlock object at 0x7fcf7bfab640>, <__main__.BasicBlock object at 0x7fcf7bfc2d40>, <keras.src.engine.sequential.Sequential object at 0x7fcf7bfe88b0>, <__main__.PatchMerging object at 0x7fcf7bfe8e20>, <keras.src.engine.sequential.Sequential object at 0x7fcf7b09a8c0>]\n",
      "7 10 6\n",
      "Stage 1: \n",
      "block1 in:  (1, 32, 32, 224)\n",
      "block1 out (nach Downsampling):  (1, 16, 16, 336)\n",
      "\n",
      "Stage 2: \n",
      "block2 in:  (1, 16, 16, 336)\n",
      "block2 out (nach Downsampling):  (1, 8, 8, 448)\n",
      "\n",
      "Stage 3: \n",
      "block3 in:  (1, 8, 8, 448)\n",
      "block3 out:  (1, 8, 8, 448)\n"
     ]
    }
   ],
   "source": [
    "for i, (ed, kd, pd, dpth, do, t) in enumerate(zip(embed_dim, qk_dim, partial_dim, depth, down_ops, types)):\n",
    "    print (i, ed, kd, pd, dpth, do, t)\n",
    "    print(\"dpth: \", dpth)\n",
    "    for d in range(dpth):\n",
    "        eval(\"blocks\" + str(i+1)).append(BasicBlock(ed, kd, pd, t))\n",
    "    if do[0] == \"subsample\":\n",
    "        # Build SHViT downsample block  \n",
    "        blk = eval(\"blocks\" + str(i+1)) # vorher 2\n",
    "        blk.append(keras.Sequential([\n",
    "                    Residual(Conv2d_BN(a=embed_dim[i], filters=embed_dim[i], kernel_size=3, strides=1, padding=\"same\", groups=embed_dim[i], use_bn=train_bn)),\n",
    "                    Residual(FFN(ed=embed_dim[i], h=int(embed_dim[i] * 2), train_bn=train_bn)),\n",
    "                ]))\n",
    "        blk.append(PatchMerging(dim=embed_dim[i], out_dim=embed_dim[i + 1], train_bn=train_bn))\n",
    "        # print(\"PatchMerging: 1\" )\n",
    "        # print(\"PatchMerging: \", blk)\n",
    "        # print(\"PatchMerging: 2\" )\n",
    "\n",
    "        blk.append(keras.Sequential([\n",
    "                    Residual(Conv2d_BN(a=embed_dim[i + 1], filters=embed_dim[i + 1], kernel_size=3, strides=1, padding=\"same\", groups=embed_dim[i + 1], use_bn=train_bn)),\n",
    "                    Residual(FFN(ed=embed_dim[i + 1], h=int(embed_dim[i + 1] * 2), train_bn=train_bn)),\n",
    "                ]))\n",
    "\n",
    "print(eval('blocks1'))\n",
    "\n",
    "print(len(blocks1),len(blocks2),len(blocks3)) # 4, 7+3, 6+3\n",
    "\n",
    "blocks1 = tf.keras.Sequential(blocks1)\n",
    "blocks2 = tf.keras.Sequential(blocks2)\n",
    "blocks3 = tf.keras.Sequential(blocks3)\n",
    "\n",
    "print(\"Stage 1: \")\n",
    "print(\"block1 in: \", x.shape)\n",
    "x = blocks1(x)\n",
    "outs.append(x)\n",
    "print(\"block1 out (nach Downsampling): \", x.shape)\n",
    "\n",
    "print()\n",
    "print(\"Stage 2: \")\n",
    "print(\"block2 in: \", x.shape)\n",
    "x = blocks2(x)\n",
    "outs.append(x)\n",
    "print(\"block2 out (nach Downsampling): \", x.shape)\n",
    "\n",
    "print()\n",
    "print(\"Stage 3: \")\n",
    "print(\"block3 in: \", x.shape)\n",
    "x = blocks3(x)\n",
    "outs.append(x)\n",
    "print(\"block3 out: \", x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TensorShape([1, 64, 64, 112]), TensorShape([1, 64, 64, 112]), TensorShape([1, 16, 16, 336]), TensorShape([1, 8, 8, 448]), TensorShape([1, 8, 8, 448])]\n"
     ]
    }
   ],
   "source": [
    "shapes = [tensor.shape for tensor in outs]\n",
    "print(shapes)\n",
    "\n",
    "_, C2, C3, C4, C5 = outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    # Size of the top-down layers used to build the feature pyramid\n",
    "    TOP_DOWN_PYRAMID_SIZE = 256\n",
    "    \n",
    "    # The strides of each layer of the FPN Pyramid. These values\n",
    "    # are based on a SHViT  backbone.\n",
    "    BACKBONE_STRIDES = [16, 32, 64, 128] # 128 added as they add 64 in the paper (original strides were [4,8,16,32] -> [4,8,16,32,64], maybe because P5 was upsampled by factor 2??)\n",
    "    \n",
    "    # Length of square anchor side in pixels\n",
    "    RPN_ANCHOR_SCALES = (32, 64, 128, 256) \n",
    "\n",
    "    # Ratios of anchors at each cell (width/height)\n",
    "    # A value of 1 represents a square anchor, and 0.5 is a wide anchor\n",
    "    RPN_ANCHOR_RATIOS = [0.5, 1, 2]\n",
    "\n",
    "    # Anchor stride\n",
    "    # If 1 then anchors are created for each cell in the backbone feature map.\n",
    "    # If 2, then anchors are created for every other cell, and so on.\n",
    "    RPN_ANCHOR_STRIDE = 1    \n",
    "    \n",
    "\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Inputs have incompatible shapes. Received shapes (32, 32, 256) and (64, 64, 256)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m P4 \u001b[38;5;241m=\u001b[39m KL\u001b[38;5;241m.\u001b[39mConv2D(filters\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mTOP_DOWN_PYRAMID_SIZE, kernel_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfpn_c4p4\u001b[39m\u001b[38;5;124m'\u001b[39m)(C4)\n\u001b[1;32m      2\u001b[0m P3 \u001b[38;5;241m=\u001b[39m KL\u001b[38;5;241m.\u001b[39mAdd(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfpn_p3add\u001b[39m\u001b[38;5;124m\"\u001b[39m)([\n\u001b[1;32m      3\u001b[0m             KL\u001b[38;5;241m.\u001b[39mUpSampling2D(size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m), name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfpn_p4upsampled\u001b[39m\u001b[38;5;124m\"\u001b[39m)(P4),\n\u001b[1;32m      4\u001b[0m             KL\u001b[38;5;241m.\u001b[39mConv2D(filters\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mTOP_DOWN_PYRAMID_SIZE, kernel_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfpn_c3p3\u001b[39m\u001b[38;5;124m'\u001b[39m)(C3)])\n\u001b[0;32m----> 5\u001b[0m P2 \u001b[38;5;241m=\u001b[39m \u001b[43mKL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfpn_p2add\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m            \u001b[49m\u001b[43mKL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUpSampling2D\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfpn_p3upsampled\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mP3\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m            \u001b[49m\u001b[43mKL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConv2D\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTOP_DOWN_PYRAMID_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfpn_c2p2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mC2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(P4\u001b[38;5;241m.\u001b[39mshape, P3\u001b[38;5;241m.\u001b[39mshape, P2\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf215/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/tf215/lib/python3.10/site-packages/keras/src/layers/merging/base_merge.py:74\u001b[0m, in \u001b[0;36m_Merge._compute_elemwise_op_output_shape\u001b[0;34m(self, shape1, shape2)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     73\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m j:\n\u001b[0;32m---> 74\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     75\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInputs have incompatible shapes. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     76\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape2\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     77\u001b[0m             )\n\u001b[1;32m     78\u001b[0m         output_shape\u001b[38;5;241m.\u001b[39mappend(i)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(output_shape)\n",
      "\u001b[0;31mValueError\u001b[0m: Inputs have incompatible shapes. Received shapes (32, 32, 256) and (64, 64, 256)"
     ]
    }
   ],
   "source": [
    "P4 = KL.Conv2D(filters=config.TOP_DOWN_PYRAMID_SIZE, kernel_size=(1, 1), name='fpn_c4p4')(C4)\n",
    "P3 = KL.Add(name=\"fpn_p3add\")([\n",
    "            KL.UpSampling2D(size=(2, 2), name=\"fpn_p4upsampled\")(P4),\n",
    "            KL.Conv2D(filters=config.TOP_DOWN_PYRAMID_SIZE, kernel_size=(1, 1), name='fpn_c3p3')(C3)])\n",
    "P2 = KL.Add(name=\"fpn_p2add\")([\n",
    "            KL.UpSampling2D(size=(2, 2), name=\"fpn_p3upsampled\")(P3),\n",
    "            KL.Conv2D(filters=config.TOP_DOWN_PYRAMID_SIZE, kernel_size=(1, 1), name='fpn_c2p2')(C2)])\n",
    "\n",
    "print(P4.shape, P3.shape, P2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach 3x3 conv to all P layers to get the final feature maps.\n",
    "\n",
    "P2 = KL.Conv2D(filters=config.TOP_DOWN_PYRAMID_SIZE, kernel_size=(3, 3), padding=\"SAME\", name=\"fpn_p2\")(P2)\n",
    "P3 = KL.Conv2D(filters=config.TOP_DOWN_PYRAMID_SIZE, kernel_size=(3, 3), padding=\"SAME\", name=\"fpn_p3\")(P3)\n",
    "P4 = KL.Conv2D(filters=config.TOP_DOWN_PYRAMID_SIZE, kernel_size=(3, 3), padding=\"SAME\", name=\"fpn_p4\")(P4)\n",
    "P5 = KL.MaxPooling2D(pool_size=(1, 1), strides=2, name=\"fpn_p5\")(P4) # Here we introduce P5 only for covering a larger anchor scale of 256^2. P5 is simply a stride two subsampling of P4. (footnote page 4 of https://arxiv.org/pdf/1612.03144)\n",
    "\n",
    "rpn_feature_maps = [P2, P3, P4, P5]\n",
    "mrcnn_feature_maps = [P2, P3, P4]\n",
    "\n",
    "print(P5.shape, P4.shape, P3.shape, P2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_shape = input_image[0, :, :, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_backbone_shapes(config, image_shape):\n",
    "    \"\"\"Computes the width and height of each stage of the backbone network.\n",
    "\n",
    "    Returns:\n",
    "        [N, (height, width)]. Where N is the number of stages\n",
    "    \"\"\"\n",
    "\n",
    "    x = np.array(\n",
    "        [[int(math.ceil(image_shape[0] / stride)),\n",
    "            int(math.ceil(image_shape[1] / stride))]\n",
    "            for stride in config.BACKBONE_STRIDES])\n",
    "\n",
    "    return x\n",
    "\n",
    "backbone_shapes = compute_backbone_shapes(config, image_shape)\n",
    "print(\"backbone_shapes: \", backbone_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "#  Anchors\n",
    "############################################################\n",
    "\n",
    "def generate_anchors(scales, ratios, shape, feature_stride, anchor_stride):\n",
    "    \"\"\"\n",
    "    scales: 1D array of anchor sizes in pixels. Example: [32, 64, 128]\n",
    "    ratios: 1D array of anchor ratios of width/height. Example: [0.5, 1, 2]\n",
    "    shape: [height, width] spatial shape of the feature map over which\n",
    "            to generate anchors.\n",
    "    feature_stride: Stride of the feature map relative to the image in pixels.\n",
    "    anchor_stride: Stride of anchors on the feature map. For example, if the\n",
    "        value is 2 then generate anchors for every other feature map pixel.\n",
    "    \"\"\"\n",
    "    # Get all combinations of scales and ratios\n",
    "    scales, ratios = np.meshgrid(np.array(scales), np.array(ratios))\n",
    "    scales = scales.flatten()\n",
    "    ratios = ratios.flatten()\n",
    "\n",
    "    # Enumerate heights and widths from scales and ratios\n",
    "    heights = scales / np.sqrt(ratios)\n",
    "    widths = scales * np.sqrt(ratios)\n",
    "\n",
    "    # Enumerate shifts in feature space\n",
    "    shifts_y = np.arange(0, shape[0], anchor_stride) * feature_stride\n",
    "    shifts_x = np.arange(0, shape[1], anchor_stride) * feature_stride\n",
    "    shifts_x, shifts_y = np.meshgrid(shifts_x, shifts_y)\n",
    "\n",
    "    # Enumerate combinations of shifts, widths, and heights\n",
    "    box_widths, box_centers_x = np.meshgrid(widths, shifts_x)\n",
    "    box_heights, box_centers_y = np.meshgrid(heights, shifts_y)\n",
    "\n",
    "    # Reshape to get a list of (y, x) and a list of (h, w)\n",
    "    box_centers = np.stack(\n",
    "        [box_centers_y, box_centers_x], axis=2).reshape([-1, 2])\n",
    "    box_sizes = np.stack([box_heights, box_widths], axis=2).reshape([-1, 2])\n",
    "\n",
    "    # Convert to corner coordinates (y1, x1, y2, x2)\n",
    "    boxes = np.concatenate([box_centers - 0.5 * box_sizes,\n",
    "                            box_centers + 0.5 * box_sizes], axis=1)\n",
    "    return boxes\n",
    "\n",
    "\n",
    "def generate_pyramid_anchors(scales, ratios, feature_shapes, feature_strides,\n",
    "                             anchor_stride):\n",
    "    \"\"\"Generate anchors at different levels of a feature pyramid. Each scale\n",
    "    is associated with a level of the pyramid, but each ratio is used in\n",
    "    all levels of the pyramid.\n",
    "\n",
    "    Returns:\n",
    "    anchors: [N, (y1, x1, y2, x2)]. All generated anchors in one array. Sorted\n",
    "        with the same order of the given scales. So, anchors of scale[0] come\n",
    "        first, then anchors of scale[1], and so on.\n",
    "    \"\"\"\n",
    "    # Anchors\n",
    "    # [anchor_count, (y1, x1, y2, x2)]\n",
    "    print(len(scales))\n",
    "    anchors = []\n",
    "    for i in range(len(scales)):\n",
    "        #print(i, anchors)\n",
    "        anchors.append(generate_anchors(scales[i], ratios, feature_shapes[i],\n",
    "                                        feature_strides[i], anchor_stride))\n",
    "    return np.concatenate(anchors, axis=0)\n",
    "\n",
    "\n",
    "a = generate_pyramid_anchors(\n",
    "                config.RPN_ANCHOR_SCALES,\n",
    "                config.RPN_ANCHOR_RATIOS,\n",
    "                backbone_shapes,\n",
    "                config.BACKBONE_STRIDES,\n",
    "                config.RPN_ANCHOR_STRIDE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RPN_ANCHOR_SCALES\", config.RPN_ANCHOR_SCALES)\n",
    "print(\"RPN_ANCHOR_RATIOS\", config.RPN_ANCHOR_RATIOS)\n",
    "print(\"backbone_shapes\", backbone_shapes)\n",
    "print(\"BACKBONE_STRIDES\", config.BACKBONE_STRIDES)\n",
    "print(\"RPN_ANCHOR_STRIDE\", config.RPN_ANCHOR_STRIDE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf215",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
