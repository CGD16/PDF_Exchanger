{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from models_torch.utils import ResizeLayer, DropPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 64, 64])\n",
      "ResizeLayer test passed.\n"
     ]
    }
   ],
   "source": [
    "def test_resize_layer():\n",
    "    # Define the target size\n",
    "    target_height = 64\n",
    "    target_width = 64\n",
    "    \n",
    "    # Create an instance of ResizeLayer\n",
    "    resize_layer = ResizeLayer(target_height, target_width)\n",
    "    \n",
    "    # Create a sample input tensor with a different size\n",
    "    input_tensor = torch.randn(1, 3, 32, 32)  # Batch size of 1, 3 channels, 32x32 image\n",
    "    \n",
    "    # Perform the resizing operation\n",
    "    output_tensor = resize_layer(input_tensor)\n",
    "    \n",
    "    # Check the output size\n",
    "    assert output_tensor.shape == (1, 3, target_height, target_width), \"Output shape should match the target size.\"\n",
    "    print(output_tensor.shape)\n",
    "    \n",
    "    print(\"ResizeLayer test passed.\")\n",
    "\n",
    "# Run the test\n",
    "test_resize_layer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of zero elements during training: 2994\n",
      "torch.Size([5, 3, 32, 32])\n",
      "All tests passed.\n"
     ]
    }
   ],
   "source": [
    "def test_drop_path():\n",
    "    drop_path_rate = 0.2\n",
    "    module = DropPath(drop_path_rate)\n",
    "    x = torch.ones(5, 3, 32, 32)  # Example input tensor\n",
    "\n",
    "    # Test during training\n",
    "    output_training = module(x)\n",
    "    \n",
    "    zero_elements = 0\n",
    "    while zero_elements == 0:\n",
    "        # Test during training\n",
    "        output_training = module(x)\n",
    "        zero_elements = (output_training == 0).sum().item()\n",
    "        \n",
    "    print(f\"Number of zero elements during training: {zero_elements}\")\n",
    " \n",
    "    assert (output_training == 0).any().item(), \"Some elements should be zeroed out during training.\"\n",
    "    assert output_training.shape == x.shape, \"Output shape should match input shape during training.\"\n",
    "    print(output_training.shape)\n",
    "\n",
    "    print(\"All tests passed.\")\n",
    "\n",
    "# Run the test\n",
    "test_drop_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from models_torch.attention import Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 64, 64])\n",
      "Attention test passed.\n"
     ]
    }
   ],
   "source": [
    "def test_attention():\n",
    "    # Define parameters\n",
    "    dim = 64\n",
    "    num_heads = 8\n",
    "    sr_ratio = 1\n",
    "    batch_size = 2\n",
    "    height = 8\n",
    "    width = 8\n",
    "\n",
    "    # Create an instance of Attention\n",
    "    attention_layer = Attention(dim, num_heads, sr_ratio)\n",
    "\n",
    "    # Create a sample input tensor\n",
    "    input_tensor = torch.randn(batch_size, height * width, dim)  # B, N, C\n",
    "\n",
    "    # Perform the attention operation\n",
    "    output_tensor = attention_layer(input_tensor, height, width)\n",
    "\n",
    "    # Check the output size\n",
    "    assert output_tensor.shape == (batch_size, height * width, dim), \"Output shape should match the input shape.\"\n",
    "    print(output_tensor.shape)\n",
    "\n",
    "    print(\"Attention test passed.\")\n",
    "\n",
    "# Run the test\n",
    "test_attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from models_torch.head import MLP, ConvModule, SegFormerHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of input:  torch.Size([4, 256])\n",
      "torch.Size([4, 128])\n",
      "MLP test passed.\n"
     ]
    }
   ],
   "source": [
    "def test_mlp():\n",
    "    # Define the dimension\n",
    "    input_dim = 256\n",
    "    decode_dim = 128\n",
    "    \n",
    "    # Create an instance of MLP\n",
    "    mlp_layer = MLP(input_dim, decode_dim)\n",
    "    \n",
    "    # Create a sample input tensor\n",
    "    batch_size = 4\n",
    "    input_tensor = torch.randn(batch_size, input_dim)  # B, decode_dim\n",
    "    print(\"shape of input: \", input_tensor.shape)\n",
    "\n",
    "    \n",
    "    # Perform the MLP operation\n",
    "    output_tensor = mlp_layer(input_tensor)\n",
    "    \n",
    "    # Check the output size\n",
    "    assert output_tensor.shape == (batch_size, decode_dim), \"Output shape should match the input shape.\"\n",
    "    print(output_tensor.shape)\n",
    "    \n",
    "    print(\"MLP test passed.\")\n",
    "\n",
    "# Run the test\n",
    "test_mlp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 64, 32, 32])\n",
      "ConvModule test passed.\n"
     ]
    }
   ],
   "source": [
    "def test_conv_module():\n",
    "    # Define the dimension\n",
    "    decode_dim_in = 4*64\n",
    "    decode_dim_out = 64\n",
    "    \n",
    "    # Create an instance of ConvModule\n",
    "    conv_module = ConvModule(decode_dim_in, decode_dim_out)\n",
    "    \n",
    "    # Create a sample input tensor\n",
    "    batch_size = 4\n",
    "    height = 32\n",
    "    width = 32\n",
    "    input_tensor = torch.randn(batch_size, decode_dim_in, height, width)  # B, C, H, W\n",
    "    \n",
    "    # Perform the ConvModule operation in training mode\n",
    "    output_tensor_training = conv_module(input_tensor)\n",
    "    \n",
    "    # Check the output size in training mode\n",
    "    assert output_tensor_training.shape == (batch_size, decode_dim_out, height, width), \"Output shape should match the input shape in training mode.\"\n",
    "    print(output_tensor_training.shape)\n",
    "    \n",
    "   \n",
    "    print(\"ConvModule test passed.\")\n",
    "# Run the test\n",
    "test_conv_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 19, 32, 32])\n",
      "SegFormerHead test passed.\n"
     ]
    }
   ],
   "source": [
    "def test_segformer_head():\n",
    "    # Define parameters\n",
    "    input_dims = [64, 128, 320, 512]\n",
    "    decode_dim = 768\n",
    "    num_classes = 19\n",
    "    batch_size = 2\n",
    "    height = 32\n",
    "    width = 32\n",
    "\n",
    "    # Create an instance of SegFormerHead\n",
    "    segformer_head = SegFormerHead(input_dims, decode_dim, num_classes)\n",
    "\n",
    "    # Create sample input tensors\n",
    "    inputs = [torch.randn(batch_size, dim, height, width) for dim in input_dims]\n",
    "\n",
    "    # Perform the SegFormerHead operation in training mode\n",
    "    output_tensor_training = segformer_head(inputs)\n",
    "\n",
    "    # Check the output size in training mode\n",
    "    assert output_tensor_training.shape == (batch_size, num_classes, height, width), \"Output shape should match the expected shape in training mode.\"\n",
    "    print(output_tensor_training.shape)\n",
    "    \n",
    "    print(\"SegFormerHead test passed.\")\n",
    "\n",
    "# Run the test\n",
    "test_segformer_head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from models_torch.modules import DWConv, Mlp, Block, OverlapPatchEmbed, MixVisionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 1024, 768])\n",
      "Test passed successfully!\n"
     ]
    }
   ],
   "source": [
    "def test_dwconv():\n",
    "    # Initialize the module\n",
    "    hidden_features = 768\n",
    "    dwconv = DWConv(hidden_features)\n",
    "    \n",
    "    # Create mock input data\n",
    "    batch_size = 2\n",
    "    height, width = 32, 32\n",
    "    input_tensor = torch.randn(batch_size, height * width, hidden_features)\n",
    "    \n",
    "    # Test forward pass\n",
    "    output = dwconv(input_tensor, height, width)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    \n",
    "    # Check output shape\n",
    "    assert output.shape == (batch_size, height * width, hidden_features), \"Output shape mismatch\"\n",
    "    print(\"Test passed successfully!\")\n",
    "    \n",
    "# Run the test function\n",
    "test_dwconv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape in training mode: torch.Size([1, 1024, 256])\n",
      "Tests passed successfully!\n"
     ]
    }
   ],
   "source": [
    "def test_mlp():\n",
    "    # Initialize the module\n",
    "    in_features = 768\n",
    "    hidden_features = 512\n",
    "    out_features = 256\n",
    "    drop_rate = 0.1\n",
    "    mlp = Mlp(in_features, hidden_features, out_features, drop_rate)\n",
    "    \n",
    "    # Create mock input data\n",
    "    batch_size = 1\n",
    "    height, width = 32, 32\n",
    "    input_tensor = torch.randn(batch_size, height * width, in_features)\n",
    "    \n",
    "    # Test forward pass in training mode\n",
    "    output_training = mlp(input_tensor, height, width)\n",
    "    print(\"Output shape in training mode:\", output_training.shape)\n",
    "    \n",
    "    # Check output shape\n",
    "    assert output_training.shape == (batch_size, height * width, out_features), \"Output shape mismatch in training mode\"\n",
    "    print(\"Tests passed successfully!\")\n",
    "    \n",
    "# Run the test function\n",
    "test_mlp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape in training mode: torch.Size([1, 1024, 768])\n",
      "Tests passed successfully!\n"
     ]
    }
   ],
   "source": [
    "def test_block():\n",
    "    # Initialize the module\n",
    "    dim = 768\n",
    "    num_heads = 12\n",
    "    mlp_ratio = 4.0\n",
    "    qkv_bias = True\n",
    "    drop = 0.1\n",
    "    attn_drop = 0.1\n",
    "    drop_path = 0.1\n",
    "    sr_ratio = 1\n",
    "    block = Block(dim, num_heads, mlp_ratio, qkv_bias, drop, attn_drop, drop_path, sr_ratio)\n",
    "\n",
    "    # Create mock input data\n",
    "    batch_size = 1\n",
    "    height, width = 32, 32\n",
    "    input_tensor = torch.randn(batch_size, height * width, dim)\n",
    "\n",
    "    # Test forward pass in training mode\n",
    "    output_training = block(input_tensor, height, width)\n",
    "    print(\"Output shape in training mode:\", output_training.shape)\n",
    "\n",
    "    # Check output shape\n",
    "    assert output_training.shape == (batch_size, height * width, dim), \"Output shape mismatch in training mode\"\n",
    "\n",
    "    print(\"Tests passed successfully!\")\n",
    "\n",
    "# Run the test function\n",
    "test_block()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 3136, 768])\n",
      "Height: 56\n",
      "Width: 56\n",
      "Tests passed successfully!\n"
     ]
    }
   ],
   "source": [
    "def test_overlap_patch_embed():\n",
    "    # Initialize the module\n",
    "    img_size = 224\n",
    "    img_channels = 3\n",
    "    patch_size = 7\n",
    "    stride = 4\n",
    "    filters = 768\n",
    "    overlap_patch_embed = OverlapPatchEmbed(img_size, img_channels, patch_size, stride, filters)\n",
    "    \n",
    "    # Create mock input data\n",
    "    batch_size = 1\n",
    "    input_tensor = torch.randn(batch_size, img_channels, img_size, img_size)  # Simulate a batch of images\n",
    "    \n",
    "    # Test forward pass\n",
    "    output, H, W = overlap_patch_embed(input_tensor)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    print(\"Height:\", H)\n",
    "    print(\"Width:\", W)\n",
    "    \n",
    "    # Calculate expected dimensions\n",
    "    expected_H = (img_size + patch_size // 2 * 2 - patch_size) // stride + 1\n",
    "    expected_W = expected_H  # Assuming square input\n",
    "    \n",
    "    # Check output shape\n",
    "    assert output.shape == (batch_size, expected_H * expected_W, filters), \"Output shape mismatch\"\n",
    "    assert H == expected_H, \"Height mismatch\"\n",
    "    assert W == expected_W, \"Width mismatch\"\n",
    "    print(\"Tests passed successfully!\")\n",
    "    \n",
    "# Run the test function\n",
    "test_overlap_patch_embed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization test passed.\n",
      "torch.Size([1, 64, 56, 56])\n",
      "torch.Size([1, 128, 28, 28])\n",
      "torch.Size([1, 256, 14, 14])\n",
      "torch.Size([1, 512, 7, 7])\n",
      "Forward pass test passed.\n"
     ]
    }
   ],
   "source": [
    "def test_mix_vision_transformer():\n",
    "    # Initialize the MixVisionTransformer with default parameters\n",
    "    model = MixVisionTransformer()\n",
    "\n",
    "    # Test initialization\n",
    "    assert isinstance(model, nn.Module), \"Model is not an instance of nn.Module\"\n",
    "    assert len(model.patch_embeds) == 4, \"Incorrect number of patch embeddings\"\n",
    "    assert len(model.blocks) == 4, \"Incorrect number of blocks\"\n",
    "    assert len(model.norms) == 4, \"Incorrect number of norms\"\n",
    "\n",
    "    print(\"Initialization test passed.\")\n",
    "\n",
    "    # Create a dummy input tensor with the shape (batch_size, channels, height, width)\n",
    "    dummy_input = torch.randn(1, 3, 224, 224)  # Batch size of 1, 3 channels, 224x224 image\n",
    "\n",
    "    # Perform a forward pass\n",
    "    output = model(dummy_input)\n",
    "\n",
    "    # Check if the output is a list and has the expected number of feature maps\n",
    "    assert isinstance(output, list), \"Output is not a list\"\n",
    "    assert len(output) == 4, \"Output does not have 4 feature maps\"\n",
    "\n",
    "    # Check the shape of each feature map\n",
    "    expected_shapes = [(1, 64, 56, 56), (1, 128, 28, 28), (1, 256, 14, 14), (1, 512, 7, 7)]\n",
    "    for out, expected_shape in zip(output, expected_shapes):\n",
    "        print(out.shape)\n",
    "        assert out.shape == expected_shape, f\"Feature map shape {out.shape} does not match expected {expected_shape}\"\n",
    "\n",
    "    print(\"Forward pass test passed.\")\n",
    "    \n",
    "    \n",
    "test_mix_vision_transformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "GELU.forward() missing 1 required positional argument: 'input'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     22\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTest passed!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Run the test\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[43mtest_segformer_b0\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mtest_segformer_b0\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      7\u001b[39m input_shape = (\u001b[32m3\u001b[39m, \u001b[32m272\u001b[39m, \u001b[32m272\u001b[39m)  \u001b[38;5;66;03m# Example input shape (channels, height, width)\u001b[39;00m\n\u001b[32m      8\u001b[39m num_classes = \u001b[32m10\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m model = \u001b[43mSegFormer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mB0\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_resize\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Create a dummy input tensor\u001b[39;00m\n\u001b[32m     12\u001b[39m dummy_input = torch.rand(\u001b[32m1\u001b[39m, *input_shape)  \u001b[38;5;66;03m# Batch size of 1\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/ChangGeng/Desktop/BA-Arbeit/PDF_Exchanger/SHVIT/20250405_shoes_segformer/models_torch/segformer.py:100\u001b[39m, in \u001b[36mSegFormer.__init__\u001b[39m\u001b[34m(self, model_type, shvit_type, input_shape, num_convs, num_stages, num_classes, use_resize)\u001b[39m\n\u001b[32m     97\u001b[39m model_type = model_type.lower()\n\u001b[32m     98\u001b[39m \u001b[38;5;28mself\u001b[39m.use_resize = use_resize\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m \u001b[38;5;28mself\u001b[39m.mix_vision_transformer = \u001b[43mMixVisionTransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimg_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_channels\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m    \u001b[49m\u001b[43membed_dims\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMODEL_CONFIGS\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmit_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodel_type\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43membed_dims\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdepths\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMODEL_CONFIGS\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmit_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodel_type\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdepths\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[38;5;28mself\u001b[39m.seg_former_head = SegFormerHead(\n\u001b[32m    107\u001b[39m     num_classes=num_classes,\n\u001b[32m    108\u001b[39m     decode_dim=MODEL_CONFIGS[\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmit_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mdecode_dim\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    109\u001b[39m     input_dims=MODEL_CONFIGS[\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmit_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33membed_dims\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    110\u001b[39m )\n\u001b[32m    112\u001b[39m \u001b[38;5;28mself\u001b[39m.resize_layer = ResizeLayer(input_shape[\u001b[32m1\u001b[39m], input_shape[\u001b[32m2\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/ChangGeng/Desktop/BA-Arbeit/PDF_Exchanger/SHVIT/20250405_shoes_segformer/models_torch/modules.py:205\u001b[39m, in \u001b[36mMixVisionTransformer.__init__\u001b[39m\u001b[34m(self, img_size, img_channels, embed_dims, num_heads, mlp_ratios, qkv_bias, drop_rate, attn_drop_rate, drop_path_rate, depths, sr_ratios)\u001b[39m\n\u001b[32m    195\u001b[39m \u001b[38;5;28mself\u001b[39m.patch_embeds = nn.ModuleList([\n\u001b[32m    196\u001b[39m     OverlapPatchEmbed(img_size=img_sizes[i], img_channels=channels[i], patch_size=patch_sizes[i],\n\u001b[32m    197\u001b[39m                       stride=strides[i], filters=embed_dims[i])\n\u001b[32m    198\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(embed_dims))\n\u001b[32m    199\u001b[39m ])\n\u001b[32m    201\u001b[39m dpr = torch.linspace(\u001b[32m0.0\u001b[39m, drop_path_rate, \u001b[38;5;28msum\u001b[39m(depths)).tolist()\n\u001b[32m    203\u001b[39m \u001b[38;5;28mself\u001b[39m.blocks = nn.ModuleList([\n\u001b[32m    204\u001b[39m     nn.ModuleList([\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m         \u001b[43mBlock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[43membed_dims\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlp_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmlp_ratios\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqkv_bias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mqkv_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m              \u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdrop_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_drop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattn_drop_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrop_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdpr\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdepths\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m              \u001b[49m\u001b[43msr_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[43msr_ratios\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    208\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(depths[i])\n\u001b[32m    209\u001b[39m     ])\n\u001b[32m    210\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(embed_dims))\n\u001b[32m    211\u001b[39m ])\n\u001b[32m    213\u001b[39m \u001b[38;5;28mself\u001b[39m.norms = nn.ModuleList([nn.LayerNorm(embed_dims[i], eps=\u001b[32m1e-5\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(embed_dims))])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/ChangGeng/Desktop/BA-Arbeit/PDF_Exchanger/SHVIT/20250405_shoes_segformer/models_torch/modules.py:116\u001b[39m, in \u001b[36mBlock.__init__\u001b[39m\u001b[34m(self, dim, num_heads, mlp_ratio, qkv_bias, drop, attn_drop, drop_path, sr_ratio)\u001b[39m\n\u001b[32m    114\u001b[39m \u001b[38;5;28mself\u001b[39m.drop_path = DropPath(drop_path)\n\u001b[32m    115\u001b[39m \u001b[38;5;28mself\u001b[39m.norm2 = nn.LayerNorm(dim, eps=\u001b[32m1e-5\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m \u001b[38;5;28mself\u001b[39m.mlp = \u001b[43mMlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_features\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlp_ratio\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/ChangGeng/Desktop/BA-Arbeit/PDF_Exchanger/SHVIT/20250405_shoes_segformer/models_torch/modules.py:72\u001b[39m, in \u001b[36mMlp.__init__\u001b[39m\u001b[34m(self, in_features, hidden_features, out_features, act_layer, drop)\u001b[39m\n\u001b[32m     70\u001b[39m \u001b[38;5;28mself\u001b[39m.fc1 = nn.Linear(in_features, hidden_features)\n\u001b[32m     71\u001b[39m \u001b[38;5;28mself\u001b[39m.dwconv = DWConv(hidden_features)\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m \u001b[38;5;28mself\u001b[39m.act = \u001b[43mact_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[38;5;28mself\u001b[39m.fc2 = nn.Linear(hidden_features, out_features)\n\u001b[32m     74\u001b[39m \u001b[38;5;28mself\u001b[39m.drop = nn.Dropout(drop)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/torch260/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/torch260/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[31mTypeError\u001b[39m: GELU.forward() missing 1 required positional argument: 'input'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from models_torch.segformer import SegFormer, SegFormer_SHViT\n",
    "\n",
    "\n",
    "def test_segformer_b0():\n",
    "    input_shape = (3, 272, 272)  # Example input shape (channels, height, width)\n",
    "    num_classes = 10\n",
    "    model = SegFormer(model_type=\"B0\", input_shape=input_shape, num_classes=num_classes, use_resize=True)\n",
    "\n",
    "    # Create a dummy input tensor\n",
    "    dummy_input = torch.rand(1, *input_shape)  # Batch size of 1\n",
    "\n",
    "    # Perform a forward pass\n",
    "    output = model(dummy_input)\n",
    "\n",
    "    # Check the output shape\n",
    "    expected_output_shape = (1, num_classes, input_shape[1], input_shape[2])\n",
    "    print(output.shape, expected_output_shape)\n",
    "    assert output.shape == expected_output_shape, f\"Expected output shape {expected_output_shape}, but got {output.shape}\"\n",
    "\n",
    "    print(\"Test passed!\")\n",
    "\n",
    "# Run the test\n",
    "test_segformer_b0()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 136, 136]) (1, 10, 17, 17)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Expected output shape (1, 10, 17, 17), but got torch.Size([1, 10, 136, 136])",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTest passed!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Run the test\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[43mtest_segformer_shvit_b0_s4\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mtest_segformer_shvit_b0_s4\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     17\u001b[39m expected_output_shape = (\u001b[32m1\u001b[39m, num_classes, input_shape[\u001b[32m1\u001b[39m]//factor, input_shape[\u001b[32m2\u001b[39m]//factor)\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(output.shape, expected_output_shape)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m output.shape == expected_output_shape, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected output shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_output_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTest passed!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAssertionError\u001b[39m: Expected output shape (1, 10, 17, 17), but got torch.Size([1, 10, 136, 136])"
     ]
    }
   ],
   "source": [
    "def test_segformer_shvit_b0_s4():\n",
    "    input_shape = (1, 272, 272)  # Example input shape (channels, height, width)\n",
    "    num_classes = 10\n",
    "    use_resize = False\n",
    "    model = SegFormer_SHViT(model_type=\"B0\", shvit_type=\"S4\", input_shape=input_shape, num_convs=1, num_stages=3, num_classes=num_classes, use_resize=use_resize) # num_convs=4\n",
    "\n",
    "    # Create a dummy input tensor\n",
    "    dummy_input = torch.rand(1, *input_shape)  # Batch size of 1\n",
    "\n",
    "    # Perform a forward pass\n",
    "    output = model(dummy_input)\n",
    "\n",
    "    # Check the output shape\n",
    "    factor = 1\n",
    "    if not use_resize:\n",
    "        factor = 16\n",
    "    expected_output_shape = (1, num_classes, input_shape[1]//factor, input_shape[2]//factor)\n",
    "    print(output.shape, expected_output_shape)\n",
    "    assert output.shape == expected_output_shape, f\"Expected output shape {expected_output_shape}, but got {output.shape}\"\n",
    "\n",
    "    print(\"Test passed!\")\n",
    "\n",
    "# Run the test\n",
    "test_segformer_shvit_b0_s4() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_segformer_shvit_b0_s4():\n",
    "    input_shape = (1, 272, 272)  # Example input shape (channels, height, width)\n",
    "    num_classes = 10\n",
    "    use_resize = False\n",
    "    model = SegFormer_SHViT(model_type=\"B0\", shvit_type=\"S4\", input_shape=input_shape, num_stages=3, num_classes=num_classes, use_resize=use_resize)\n",
    "\n",
    "    # Create a dummy input tensor\n",
    "    dummy_input = torch.rand(1, *input_shape)  # Batch size of 1\n",
    "\n",
    "    # Perform a forward pass\n",
    "    output = model(dummy_input)\n",
    "\n",
    "    # Check the output shape\n",
    "    factor = 1\n",
    "    if not use_resize:\n",
    "        factor = 16\n",
    "    expected_output_shape = (1, num_classes, input_shape[1]//factor, input_shape[2]//factor)\n",
    "    print(output.shape, expected_output_shape)\n",
    "    assert output.shape == expected_output_shape, f\"Expected output shape {expected_output_shape}, but got {output.shape}\"\n",
    "\n",
    "    print(\"Test passed!\")\n",
    "\n",
    "# Run the test\n",
    "test_segformer_shvit_b0_s4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 56, 56, 64])\n",
      "torch.Size([1, 1, 56, 56, 64])\n",
      "torch.Size([1, 1, 56, 56, 64])\n",
      "torch.Size([1, 2, 28, 28, 64])\n",
      "torch.Size([1, 2, 28, 28, 64])\n",
      "torch.Size([1, 2, 28, 28, 64])\n",
      "torch.Size([1, 2, 28, 28, 64])\n",
      "torch.Size([1, 4, 14, 14, 64])\n",
      "torch.Size([1, 4, 14, 14, 64])\n",
      "torch.Size([1, 4, 14, 14, 64])\n",
      "torch.Size([1, 4, 14, 14, 64])\n",
      "torch.Size([1, 4, 14, 14, 64])\n",
      "torch.Size([1, 4, 14, 14, 64])\n",
      "torch.Size([1, 1000])\n",
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "# SimVit\n",
    "import torch\n",
    "from torch import nn\n",
    "from models_torch.simvit import simvit  \n",
    "\n",
    "\n",
    "def test_simvit():\n",
    "    input_shape = (3, 224, 224)  # Example input shape (channels, height, width)\n",
    "    num_classes = 10\n",
    "    use_resize = False\n",
    "    model = simvit(img_size=224, in_chans=3, num_classes=1000, embed_dims=[64, 128, 256, 512],\n",
    "                 num_heads=[1, 2, 4, 8], mlp_ratios=[4, 4, 4, 4], qkv_bias=False, qk_scale=None, drop_rate=0.,\n",
    "                 attn_drop_rate=0., drop_path_rate=0., norm_layer=nn.LayerNorm,\n",
    "                 depths=[3, 4, 6, 3], num_stages=4, linear=False)\n",
    "\n",
    "    # Create a dummy input tensor\n",
    "    dummy_input = torch.rand(1, *input_shape)  # Batch size of 1\n",
    "\n",
    "    # Perform a forward pass\n",
    "    output = model(dummy_input)\n",
    "    print(output.shape)\n",
    "\n",
    "\n",
    "    print(\"Test passed!\")\n",
    "\n",
    "# Run the test\n",
    "test_simvit()\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch260",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
