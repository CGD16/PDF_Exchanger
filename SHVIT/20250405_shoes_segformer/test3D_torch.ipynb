{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from models_torch.utils_3d import ResizeLayer3D, DropPath3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 64, 64, 64])\n",
      "ResizeLayer test passed.\n"
     ]
    }
   ],
   "source": [
    "def test_resize_layer():\n",
    "    # Define the target size\n",
    "    target_depth = 64\n",
    "    target_height = 64\n",
    "    target_width = 64\n",
    "    \n",
    "    # Create an instance of ResizeLayer\n",
    "    resize_layer = ResizeLayer3D(target_depth, target_height, target_width)\n",
    "    \n",
    "    # Create a sample input tensor with a different size\n",
    "    input_tensor = torch.randn(1, 3, 32, 32, 32)  # Batch size of 1, 3 channels, 32x32 image\n",
    "    \n",
    "    # Perform the resizing operation\n",
    "    output_tensor = resize_layer(input_tensor)\n",
    "    \n",
    "    # Check the output size\n",
    "    assert output_tensor.shape == (1, 3, target_depth, target_height, target_width), \"Output shape should match the target size.\"\n",
    "    print(output_tensor.shape)\n",
    "    \n",
    "    print(\"ResizeLayer test passed.\")\n",
    "\n",
    "# Run the test\n",
    "test_resize_layer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of zero elements during training: 97538\n",
      "torch.Size([5, 3, 32, 32, 32])\n",
      "All tests passed.\n"
     ]
    }
   ],
   "source": [
    "def test_drop_path():\n",
    "    drop_path_rate = 0.2\n",
    "    module = DropPath3D(drop_path_rate)\n",
    "    x = torch.ones(5, 3, 32, 32, 32)  # Example input tensor\n",
    "\n",
    "    # Test during training\n",
    "    output_training = module(x)\n",
    "    \n",
    "    zero_elements = 0\n",
    "    while zero_elements == 0:\n",
    "        # Test during training\n",
    "        output_training = module(x)\n",
    "        zero_elements = (output_training == 0).sum().item()\n",
    "        \n",
    "    print(f\"Number of zero elements during training: {zero_elements}\")\n",
    " \n",
    "    assert (output_training == 0).any().item(), \"Some elements should be zeroed out during training.\"\n",
    "    assert output_training.shape == x.shape, \"Output shape should match input shape during training.\"\n",
    "    print(output_training.shape)\n",
    "\n",
    "    print(\"All tests passed.\")\n",
    "\n",
    "# Run the test\n",
    "test_drop_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from models_torch.attention_3d import Attention3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 512, 64])\n",
      "Attention test passed.\n"
     ]
    }
   ],
   "source": [
    "def test_attention():\n",
    "    # Define parameters\n",
    "    dim = 64\n",
    "    num_heads = 8\n",
    "    sr_ratio = 1\n",
    "    batch_size = 2\n",
    "    depth = 8\n",
    "    height = 8\n",
    "    width = 8\n",
    "\n",
    "    # Create an instance of Attention\n",
    "    attention_layer = Attention3D(dim, num_heads, sr_ratio)\n",
    "\n",
    "    # Create a sample input tensor\n",
    "    input_tensor = torch.randn(batch_size, depth * height * width, dim)  # B, N, C\n",
    "\n",
    "    # Perform the attention operation\n",
    "    output_tensor = attention_layer(input_tensor, depth, height, width)\n",
    "\n",
    "    # Check the output size\n",
    "    assert output_tensor.shape == (batch_size, depth * height * width, dim), \"Output shape should match the input shape.\"\n",
    "    print(output_tensor.shape)\n",
    "\n",
    "    print(\"Attention test passed.\")\n",
    "\n",
    "# Run the test\n",
    "test_attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from models_torch.head_3d import MLP3D, ConvModule3D, SegFormerHead3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of input:  torch.Size([4, 5, 5, 3])\n",
      "torch.Size([4, 5, 5, 2])\n",
      "MLP test passed.\n"
     ]
    }
   ],
   "source": [
    "def test_mlp():\n",
    "    # Define the dimension\n",
    "    batch_size = 4\n",
    "    input_dim = 3\n",
    "    decode_dim = 2\n",
    "    additional_dims = (5, 5)  # Example additional dimensions\n",
    "    \n",
    "    # Create an instance of MLP\n",
    "    mlp_layer = MLP3D(input_dim, decode_dim)\n",
    "    \n",
    "    # Create a sample input tensor\n",
    "    input_tensor = torch.randn(batch_size, *additional_dims, input_dim)  # B, decode_dim\n",
    "    print(\"shape of input: \", input_tensor.shape)\n",
    "\n",
    "    \n",
    "    # Perform the MLP operation\n",
    "    output_tensor = mlp_layer(input_tensor)\n",
    "    \n",
    "    # Check the output size\n",
    "    expected_shape = (batch_size, *additional_dims, decode_dim)\n",
    "    assert output_tensor.shape == expected_shape, \"Output shape should match the input shape.\"\n",
    "    print(output_tensor.shape)\n",
    "    \n",
    "    print(\"MLP test passed.\")\n",
    "\n",
    "# Run the test\n",
    "test_mlp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 64, 32, 32, 32])\n",
      "ConvModule test passed.\n"
     ]
    }
   ],
   "source": [
    "def test_conv_module():\n",
    "    # Define the dimension\n",
    "    decode_dim_in = 4*64\n",
    "    decode_dim_out = 64\n",
    "    \n",
    "    # Create an instance of ConvModule\n",
    "    conv_module = ConvModule3D(decode_dim_in, decode_dim_out)\n",
    "    \n",
    "    # Create a sample input tensor\n",
    "    batch_size = 4\n",
    "    depth = 32\n",
    "    height = 32\n",
    "    width = 32\n",
    "    input_tensor = torch.randn(batch_size, decode_dim_in, depth, height, width)  # B, C, H, W\n",
    "    \n",
    "    # Perform the ConvModule operation in training mode\n",
    "    output_tensor_training = conv_module(input_tensor)\n",
    "    \n",
    "    # Check the output size in training mode\n",
    "    assert output_tensor_training.shape == (batch_size, decode_dim_out, depth, height, width), \"Output shape should match the input shape in training mode.\"\n",
    "    print(output_tensor_training.shape)\n",
    "    \n",
    "    print(\"ConvModule test passed.\")\n",
    "# Run the test\n",
    "test_conv_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 19, 32, 32, 32])\n",
      "SegFormerHead test passed.\n"
     ]
    }
   ],
   "source": [
    "def test_segformer_head():\n",
    "    # Define parameters\n",
    "    input_dims = [64, 128, 256, 512]\n",
    "    decode_dim = 768\n",
    "    num_classes = 19\n",
    "    batch_size = 2\n",
    "    depth = 32\n",
    "    height = 32\n",
    "    width = 32\n",
    "\n",
    "    # Create an instance of SegFormerHead\n",
    "    segformer_head = SegFormerHead3D(input_dims, decode_dim, num_classes)\n",
    "\n",
    "    # Create sample input tensors\n",
    "    inputs = [torch.randn(batch_size, dim, depth, height, width) for dim in input_dims]\n",
    "\n",
    "    # Perform the SegFormerHead operation in training mode\n",
    "    output_tensor_training = segformer_head(inputs)\n",
    "\n",
    "    # Check the output size in training mode\n",
    "    assert output_tensor_training.shape == (batch_size, num_classes, depth, height, width), \"Output shape should match the expected shape in training mode.\"\n",
    "    print(output_tensor_training.shape)\n",
    "    \n",
    "    print(\"SegFormerHead test passed.\")\n",
    "\n",
    "# Run the test\n",
    "test_segformer_head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from models_torch.modules_3d import DWConv3D, Mlp3D, Block3D, OverlapPatchEmbed3D, MixVisionTransformer3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 32768, 768])\n",
      "Test passed successfully!\n"
     ]
    }
   ],
   "source": [
    "def test_dwconv():\n",
    "    # Initialize the module\n",
    "    hidden_features = 768\n",
    "    dwconv = DWConv3D(hidden_features)\n",
    "    \n",
    "    # Create mock input data\n",
    "    batch_size = 2\n",
    "    depth, height, width = 32, 32, 32\n",
    "    input_tensor = torch.randn(batch_size, depth * height * width, hidden_features)\n",
    "    \n",
    "    # Test forward pass\n",
    "    output = dwconv(input_tensor, depth, height, width)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    \n",
    "    # Check output shape\n",
    "    assert output.shape == (batch_size, depth * height * width, hidden_features), \"Output shape mismatch\"\n",
    "    print(\"Test passed successfully!\")\n",
    "    \n",
    "# Run the test function\n",
    "test_dwconv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape in training mode: torch.Size([1, 32768, 256])\n",
      "Tests passed successfully!\n"
     ]
    }
   ],
   "source": [
    "def test_mlp():\n",
    "    # Initialize the module\n",
    "    in_features = 256\n",
    "    hidden_features = 128\n",
    "    out_features = 256\n",
    "    drop_rate = 0.1\n",
    "    mlp = Mlp3D(in_features, hidden_features, out_features, drop_rate)\n",
    "    \n",
    "    # Create mock input data\n",
    "    batch_size = 1\n",
    "    depth, height, width = 32, 32, 32\n",
    "    input_tensor = torch.randn(batch_size, depth * height * width, in_features)\n",
    "    \n",
    "    # Test forward pass in training mode\n",
    "    output_training = mlp(input_tensor, depth, height, width)\n",
    "    print(\"Output shape in training mode:\", output_training.shape)\n",
    "    \n",
    "    # Check output shape\n",
    "    assert output_training.shape == (batch_size, depth * height * width, out_features), \"Output shape mismatch in training mode\"\n",
    "    print(\"Tests passed successfully!\")\n",
    "    \n",
    "# Run the test function\n",
    "test_mlp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape in training mode: torch.Size([1, 512, 64])\n",
      "Tests passed successfully!\n"
     ]
    }
   ],
   "source": [
    "def test_block():\n",
    "    # Initialize the module\n",
    "    dim = 64\n",
    "    num_heads = 8\n",
    "    mlp_ratio = 4.0\n",
    "    qkv_bias = True\n",
    "    drop = 0.1\n",
    "    attn_drop = 0.1\n",
    "    drop_path = 0.1\n",
    "    sr_ratio = 1.0\n",
    "    block = Block3D(dim, num_heads, mlp_ratio, qkv_bias, drop, attn_drop, drop_path, sr_ratio)\n",
    "\n",
    "    # Create mock input data\n",
    "    batch_size = 1\n",
    "    depth, height, width = 8, 8, 8\n",
    "    input_tensor = torch.randn(batch_size, depth * height * width, dim)\n",
    "\n",
    "    # Test forward pass in training mode\n",
    "    output_training = block(input_tensor, depth, height, width)\n",
    "    print(\"Output shape in training mode:\", output_training.shape)\n",
    "\n",
    "    # Check output shape\n",
    "    assert output_training.shape == (batch_size, depth * height * width, dim), \"Output shape mismatch in training mode\"\n",
    "\n",
    "    print(\"Tests passed successfully!\")\n",
    "\n",
    "# Run the test function\n",
    "test_block()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 175616, 768])\n",
      "Depth: 56\n",
      "Height: 56\n",
      "Width: 56\n",
      "Tests passed successfully!\n"
     ]
    }
   ],
   "source": [
    "def test_overlap_patch_embed():\n",
    "    # Initialize the module\n",
    "    img_size = 224\n",
    "    img_channels = 3\n",
    "    patch_size = 7\n",
    "    stride = 4\n",
    "    filters = 768\n",
    "    overlap_patch_embed = OverlapPatchEmbed3D(img_size, img_channels, patch_size, stride, filters)\n",
    "    \n",
    "    # Create mock input data\n",
    "    batch_size = 1\n",
    "    input_tensor = torch.randn(batch_size, img_channels, img_size, img_size, img_size)  # Simulate a batch of images\n",
    "    \n",
    "    # Test forward pass\n",
    "    output, D, H, W = overlap_patch_embed(input_tensor)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    print(\"Depth:\", D)\n",
    "    print(\"Height:\", H)\n",
    "    print(\"Width:\", W)\n",
    "    \n",
    "    # Calculate expected dimensions\n",
    "    expected_H = (img_size + patch_size // 2 * 2 - patch_size) // stride + 1\n",
    "    expected_W = expected_H  # Assuming square input\n",
    "    expected_D = expected_H  # Assuming square input\n",
    "    \n",
    "    # Check output shape\n",
    "    assert output.shape == (batch_size, expected_D * expected_H * expected_W, filters), \"Output shape mismatch\"\n",
    "    assert H == expected_H, \"Height mismatch\"\n",
    "    assert W == expected_W, \"Width mismatch\"\n",
    "    print(\"Tests passed successfully!\")\n",
    "    \n",
    "# Run the test function\n",
    "test_overlap_patch_embed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization test passed.\n",
      "torch.Size([1, 64, 56, 56, 56])\n",
      "torch.Size([1, 128, 28, 28, 28])\n",
      "torch.Size([1, 256, 14, 14, 14])\n",
      "torch.Size([1, 512, 7, 7, 7])\n",
      "Forward pass test passed.\n"
     ]
    }
   ],
   "source": [
    "def test_mix_vision_transformer():\n",
    "    # Initialize the MixVisionTransformer with default parameters\n",
    "    model = MixVisionTransformer3D()\n",
    "\n",
    "    # Test initialization\n",
    "    assert isinstance(model, nn.Module), \"Model is not an instance of nn.Module\"\n",
    "    assert len(model.patch_embeds) == 4, \"Incorrect number of patch embeddings\"\n",
    "    assert len(model.blocks) == 4, \"Incorrect number of blocks\"\n",
    "    assert len(model.norms) == 4, \"Incorrect number of norms\"\n",
    "\n",
    "    print(\"Initialization test passed.\")\n",
    "\n",
    "    # Create a dummy input tensor with the shape (batch_size, channels, depth, height, width)\n",
    "    dummy_input = torch.randn(1, 3, 224, 224, 224)  # Batch size of 1, 3 channels, 224x224x224 image\n",
    "\n",
    "    # Perform a forward pass\n",
    "    output = model(dummy_input)\n",
    "\n",
    "    # Check if the output is a list and has the expected number of feature maps\n",
    "    assert isinstance(output, list), \"Output is not a list\"\n",
    "    assert len(output) == 4, \"Output does not have 4 feature maps\"\n",
    "\n",
    "    # Check the shape of each feature map\n",
    "    expected_shapes = [(1, 64, 56, 56, 56), (1, 128, 28, 28, 28), (1, 256, 14, 14, 14), (1, 512, 7, 7, 7)]\n",
    "    for out, expected_shape in zip(output, expected_shapes):\n",
    "        print(out.shape)\n",
    "        assert out.shape == expected_shape, f\"Feature map shape {out.shape} does not match expected {expected_shape}\"\n",
    "\n",
    "    print(\"Forward pass test passed.\")\n",
    "    \n",
    "    \n",
    "test_mix_vision_transformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from models_torch.segformer_3d import SegFormer3D, SegFormer3D_SHViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 224, 224, 224]) (1, 10, 224, 224, 224)\n",
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "def test_segformer_b0():\n",
    "    input_shape = (3, 224, 224, 224)  # Example input shape (channels, depth, height, width)\n",
    "    num_classes = 10\n",
    "    model = SegFormer3D(model_type=\"B0\", input_shape=input_shape, num_classes=num_classes, use_resize=True)\n",
    "\n",
    "    # Create a dummy input tensor\n",
    "    dummy_input = torch.rand(1, *input_shape)  # Batch size of 1\n",
    "\n",
    "    # Perform a forward pass\n",
    "    output = model(dummy_input)\n",
    "\n",
    "    # Check the output shape\n",
    "    expected_output_shape = (1, num_classes, input_shape[1], input_shape[1], input_shape[1])\n",
    "    print(output.shape, expected_output_shape)\n",
    "    assert output.shape == expected_output_shape, f\"Expected output shape {expected_output_shape}, but got {output.shape}\"\n",
    "\n",
    "    print(\"Test passed!\")\n",
    "\n",
    "# Run the test\n",
    "test_segformer_b0()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 272, 272, 272]) (1, 10, 272, 272, 272)\n",
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "def test_segformer_shvit_b0_s4():\n",
    "    input_shape = (1, 272, 272, 272)  # Example input shape (channels, depth, height, width)\n",
    "    num_classes = 10\n",
    "    use_resize = True\n",
    "    model = SegFormer3D_SHViT(model_type=\"B0\", shvit_type=\"S4\", input_shape=input_shape, num_stages=3, num_classes=num_classes, use_resize=use_resize)\n",
    "\n",
    "    # Create a dummy input tensor\n",
    "    dummy_input = torch.rand(1, *input_shape)  # Batch size of 1\n",
    "\n",
    "    # Perform a forward pass\n",
    "    output = model(dummy_input)\n",
    "\n",
    "    # Check the output shape\n",
    "    factor = 1\n",
    "    if not use_resize:\n",
    "        factor = 16\n",
    "    expected_output_shape = (1, num_classes, input_shape[1]//factor, input_shape[2]//factor, input_shape[3]//factor)\n",
    "    print(output.shape, expected_output_shape)\n",
    "    assert output.shape == expected_output_shape, f\"Expected output shape {expected_output_shape}, but got {output.shape}\"\n",
    "\n",
    "    print(\"Test passed!\")\n",
    "\n",
    "# Run the test\n",
    "test_segformer_shvit_b0_s4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 17, 17, 17]) (1, 10, 17, 17, 17)\n",
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "def test_segformer_shvit_b0_s4():\n",
    "    input_shape = (1, 272, 272, 272)  # Example input shape (channels, depth, height, width)\n",
    "    num_classes = 10\n",
    "    use_resize = False\n",
    "    model = SegFormer3D_SHViT(model_type=\"B0\", shvit_type=\"S4\", input_shape=input_shape, num_stages=3, num_classes=num_classes, use_resize=use_resize)\n",
    "\n",
    "    # Create a dummy input tensor\n",
    "    dummy_input = torch.rand(1, *input_shape)  # Batch size of 1\n",
    "\n",
    "    # Perform a forward pass\n",
    "    output = model(dummy_input)\n",
    "\n",
    "    # Check the output shape\n",
    "    factor = 1\n",
    "    if not use_resize:\n",
    "        factor = 16\n",
    "    expected_output_shape = (1, num_classes, input_shape[1]//factor, input_shape[2]//factor, input_shape[3]//factor)\n",
    "    print(output.shape, expected_output_shape)\n",
    "    assert output.shape == expected_output_shape, f\"Expected output shape {expected_output_shape}, but got {output.shape}\"\n",
    "\n",
    "    print(\"Test passed!\")\n",
    "\n",
    "# Run the test\n",
    "test_segformer_shvit_b0_s4()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch260",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
