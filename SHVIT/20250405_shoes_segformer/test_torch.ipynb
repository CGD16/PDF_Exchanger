{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "/home/cgd/miniconda3/envs/torch260/lib/python3.12/site-packages/torch/../../../libtorch_python.so: undefined symbol: _ZN5torch3jit5fuser6onednn9fuseGraphERSt10shared_ptrINS0_5GraphEE",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodels_torch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ResizeLayer, DropPath\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/torch260/lib/python3.12/site-packages/torch/__init__.py:367\u001b[39m\n\u001b[32m    365\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m USE_GLOBAL_DEPS:\n\u001b[32m    366\u001b[39m         _load_global_deps()\n\u001b[32m--> \u001b[39m\u001b[32m367\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_C\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[32m    370\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mSymInt\u001b[39;00m:\n\u001b[32m    371\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    372\u001b[39m \u001b[33;03m    Like an int (including magic methods), but redirects all operations on the\u001b[39;00m\n\u001b[32m    373\u001b[39m \u001b[33;03m    wrapped node. This is used in particular to symbolically record operations\u001b[39;00m\n\u001b[32m    374\u001b[39m \u001b[33;03m    in the symbolic shape workflow.\u001b[39;00m\n\u001b[32m    375\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: /home/cgd/miniconda3/envs/torch260/lib/python3.12/site-packages/torch/../../../libtorch_python.so: undefined symbol: _ZN5torch3jit5fuser6onednn9fuseGraphERSt10shared_ptrINS0_5GraphEE"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from models_torch.utils import ResizeLayer, DropPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_resize_layer():\n",
    "    # Define the target size\n",
    "    target_height = 64\n",
    "    target_width = 64\n",
    "    \n",
    "    # Create an instance of ResizeLayer\n",
    "    resize_layer = ResizeLayer(target_height, target_width)\n",
    "    \n",
    "    # Create a sample input tensor with a different size\n",
    "    input_tensor = torch.randn(1, 3, 32, 32)  # Batch size of 1, 3 channels, 32x32 image\n",
    "    \n",
    "    # Perform the resizing operation\n",
    "    output_tensor = resize_layer(input_tensor)\n",
    "    \n",
    "    # Check the output size\n",
    "    assert output_tensor.shape == (1, 3, target_height, target_width), \"Output shape should match the target size.\"\n",
    "    print(output_tensor.shape)\n",
    "    \n",
    "    print(\"ResizeLayer test passed.\")\n",
    "\n",
    "# Run the test\n",
    "test_resize_layer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_drop_path():\n",
    "    drop_path_rate = 0.2\n",
    "    module = DropPath(drop_path_rate)\n",
    "    x = torch.ones(5, 3, 32, 32)  # Example input tensor\n",
    "\n",
    "    # Test during training\n",
    "    output_training = module(x)\n",
    "    \n",
    "    zero_elements = 0\n",
    "    while zero_elements == 0:\n",
    "        # Test during training\n",
    "        output_training = module(x)\n",
    "        zero_elements = (output_training == 0).sum().item()\n",
    "        \n",
    "    print(f\"Number of zero elements during training: {zero_elements}\")\n",
    " \n",
    "    assert (output_training == 0).any().item(), \"Some elements should be zeroed out during training.\"\n",
    "    assert output_training.shape == x.shape, \"Output shape should match input shape during training.\"\n",
    "    print(output_training.shape)\n",
    "\n",
    "    print(\"All tests passed.\")\n",
    "\n",
    "# Run the test\n",
    "test_drop_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from models_torch.attention import Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_attention():\n",
    "    # Define parameters\n",
    "    dim = 64\n",
    "    num_heads = 8\n",
    "    sr_ratio = 1\n",
    "    batch_size = 2\n",
    "    height = 8\n",
    "    width = 8\n",
    "\n",
    "    # Create an instance of Attention\n",
    "    attention_layer = Attention(dim, num_heads, sr_ratio)\n",
    "\n",
    "    # Create a sample input tensor\n",
    "    input_tensor = torch.randn(batch_size, height * width, dim)  # B, N, C\n",
    "\n",
    "    # Perform the attention operation\n",
    "    output_tensor = attention_layer(input_tensor, height, width)\n",
    "\n",
    "    # Check the output size\n",
    "    assert output_tensor.shape == (batch_size, height * width, dim), \"Output shape should match the input shape.\"\n",
    "    print(output_tensor.shape)\n",
    "\n",
    "    print(\"Attention test passed.\")\n",
    "\n",
    "# Run the test\n",
    "test_attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from models_torch.head import MLP, ConvModule, SegFormerHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_mlp():\n",
    "    # Define the dimension\n",
    "    input_dim = 256\n",
    "    decode_dim = 128\n",
    "    \n",
    "    # Create an instance of MLP\n",
    "    mlp_layer = MLP(input_dim, decode_dim)\n",
    "    \n",
    "    # Create a sample input tensor\n",
    "    batch_size = 4\n",
    "    input_tensor = torch.randn(batch_size, input_dim)  # B, decode_dim\n",
    "    print(\"shape of input: \", input_tensor.shape)\n",
    "\n",
    "    \n",
    "    # Perform the MLP operation\n",
    "    output_tensor = mlp_layer(input_tensor)\n",
    "    \n",
    "    # Check the output size\n",
    "    assert output_tensor.shape == (batch_size, decode_dim), \"Output shape should match the input shape.\"\n",
    "    print(output_tensor.shape)\n",
    "    \n",
    "    print(\"MLP test passed.\")\n",
    "\n",
    "# Run the test\n",
    "test_mlp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_conv_module():\n",
    "    # Define the dimension\n",
    "    decode_dim_in = 4*64\n",
    "    decode_dim_out = 64\n",
    "    \n",
    "    # Create an instance of ConvModule\n",
    "    conv_module = ConvModule(decode_dim_in, decode_dim_out)\n",
    "    \n",
    "    # Create a sample input tensor\n",
    "    batch_size = 4\n",
    "    height = 32\n",
    "    width = 32\n",
    "    input_tensor = torch.randn(batch_size, decode_dim_in, height, width)  # B, C, H, W\n",
    "    \n",
    "    # Perform the ConvModule operation in training mode\n",
    "    output_tensor_training = conv_module(input_tensor)\n",
    "    \n",
    "    # Check the output size in training mode\n",
    "    assert output_tensor_training.shape == (batch_size, decode_dim_out, height, width), \"Output shape should match the input shape in training mode.\"\n",
    "    print(output_tensor_training.shape)\n",
    "    \n",
    "   \n",
    "    print(\"ConvModule test passed.\")\n",
    "# Run the test\n",
    "test_conv_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_segformer_head():\n",
    "    # Define parameters\n",
    "    input_dims = [64, 128, 320, 512]\n",
    "    decode_dim = 768\n",
    "    num_classes = 19\n",
    "    batch_size = 2\n",
    "    height = 32\n",
    "    width = 32\n",
    "\n",
    "    # Create an instance of SegFormerHead\n",
    "    segformer_head = SegFormerHead(input_dims, decode_dim, num_classes)\n",
    "\n",
    "    # Create sample input tensors\n",
    "    inputs = [torch.randn(batch_size, dim, height, width) for dim in input_dims]\n",
    "\n",
    "    # Perform the SegFormerHead operation in training mode\n",
    "    output_tensor_training = segformer_head(inputs)\n",
    "\n",
    "    # Check the output size in training mode\n",
    "    assert output_tensor_training.shape == (batch_size, num_classes, height, width), \"Output shape should match the expected shape in training mode.\"\n",
    "    print(output_tensor_training.shape)\n",
    "    \n",
    "    print(\"SegFormerHead test passed.\")\n",
    "\n",
    "# Run the test\n",
    "test_segformer_head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from models_torch.modules import DWConv, Mlp, Block, OverlapPatchEmbed, MixVisionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dwconv():\n",
    "    # Initialize the module\n",
    "    hidden_features = 768\n",
    "    dwconv = DWConv(hidden_features)\n",
    "    \n",
    "    # Create mock input data\n",
    "    batch_size = 2\n",
    "    height, width = 32, 32\n",
    "    input_tensor = torch.randn(batch_size, height * width, hidden_features)\n",
    "    \n",
    "    # Test forward pass\n",
    "    output = dwconv(input_tensor, height, width)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    \n",
    "    # Check output shape\n",
    "    assert output.shape == (batch_size, height * width, hidden_features), \"Output shape mismatch\"\n",
    "    print(\"Test passed successfully!\")\n",
    "    \n",
    "# Run the test function\n",
    "test_dwconv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_mlp():\n",
    "    # Initialize the module\n",
    "    in_features = 768\n",
    "    hidden_features = 512\n",
    "    out_features = 256\n",
    "    drop_rate = 0.1\n",
    "    mlp = Mlp(in_features, hidden_features, out_features, drop_rate)\n",
    "    \n",
    "    # Create mock input data\n",
    "    batch_size = 1\n",
    "    height, width = 32, 32\n",
    "    input_tensor = torch.randn(batch_size, height * width, in_features)\n",
    "    \n",
    "    # Test forward pass in training mode\n",
    "    output_training = mlp(input_tensor, height, width)\n",
    "    print(\"Output shape in training mode:\", output_training.shape)\n",
    "    \n",
    "    # Check output shape\n",
    "    assert output_training.shape == (batch_size, height * width, out_features), \"Output shape mismatch in training mode\"\n",
    "    print(\"Tests passed successfully!\")\n",
    "    \n",
    "# Run the test function\n",
    "test_mlp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_block():\n",
    "    # Initialize the module\n",
    "    dim = 768\n",
    "    num_heads = 12\n",
    "    mlp_ratio = 4.0\n",
    "    qkv_bias = True\n",
    "    drop = 0.1\n",
    "    attn_drop = 0.1\n",
    "    drop_path = 0.1\n",
    "    sr_ratio = 1\n",
    "    block = Block(dim, num_heads, mlp_ratio, qkv_bias, drop, attn_drop, drop_path, sr_ratio)\n",
    "\n",
    "    # Create mock input data\n",
    "    batch_size = 1\n",
    "    height, width = 32, 32\n",
    "    input_tensor = torch.randn(batch_size, height * width, dim)\n",
    "\n",
    "    # Test forward pass in training mode\n",
    "    output_training = block(input_tensor, height, width)\n",
    "    print(\"Output shape in training mode:\", output_training.shape)\n",
    "\n",
    "    # Check output shape\n",
    "    assert output_training.shape == (batch_size, height * width, dim), \"Output shape mismatch in training mode\"\n",
    "\n",
    "    print(\"Tests passed successfully!\")\n",
    "\n",
    "# Run the test function\n",
    "test_block()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_overlap_patch_embed():\n",
    "    # Initialize the module\n",
    "    img_size = 224\n",
    "    img_channels = 3\n",
    "    patch_size = 7\n",
    "    stride = 4\n",
    "    filters = 768\n",
    "    overlap_patch_embed = OverlapPatchEmbed(img_size, img_channels, patch_size, stride, filters)\n",
    "    \n",
    "    # Create mock input data\n",
    "    batch_size = 1\n",
    "    input_tensor = torch.randn(batch_size, img_channels, img_size, img_size)  # Simulate a batch of images\n",
    "    \n",
    "    # Test forward pass\n",
    "    output, H, W = overlap_patch_embed(input_tensor)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    print(\"Height:\", H)\n",
    "    print(\"Width:\", W)\n",
    "    \n",
    "    # Calculate expected dimensions\n",
    "    expected_H = (img_size + patch_size // 2 * 2 - patch_size) // stride + 1\n",
    "    expected_W = expected_H  # Assuming square input\n",
    "    \n",
    "    # Check output shape\n",
    "    assert output.shape == (batch_size, expected_H * expected_W, filters), \"Output shape mismatch\"\n",
    "    assert H == expected_H, \"Height mismatch\"\n",
    "    assert W == expected_W, \"Width mismatch\"\n",
    "    print(\"Tests passed successfully!\")\n",
    "    \n",
    "# Run the test function\n",
    "test_overlap_patch_embed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_mix_vision_transformer():\n",
    "    # Initialize the MixVisionTransformer with default parameters\n",
    "    model = MixVisionTransformer()\n",
    "\n",
    "    # Test initialization\n",
    "    assert isinstance(model, nn.Module), \"Model is not an instance of nn.Module\"\n",
    "    assert len(model.patch_embeds) == 4, \"Incorrect number of patch embeddings\"\n",
    "    assert len(model.blocks) == 4, \"Incorrect number of blocks\"\n",
    "    assert len(model.norms) == 4, \"Incorrect number of norms\"\n",
    "\n",
    "    print(\"Initialization test passed.\")\n",
    "\n",
    "    # Create a dummy input tensor with the shape (batch_size, channels, height, width)\n",
    "    dummy_input = torch.randn(1, 3, 224, 224)  # Batch size of 1, 3 channels, 224x224 image\n",
    "\n",
    "    # Perform a forward pass\n",
    "    output = model(dummy_input)\n",
    "\n",
    "    # Check if the output is a list and has the expected number of feature maps\n",
    "    assert isinstance(output, list), \"Output is not a list\"\n",
    "    assert len(output) == 4, \"Output does not have 4 feature maps\"\n",
    "\n",
    "    # Check the shape of each feature map\n",
    "    expected_shapes = [(1, 64, 56, 56), (1, 128, 28, 28), (1, 256, 14, 14), (1, 512, 7, 7)]\n",
    "    for out, expected_shape in zip(output, expected_shapes):\n",
    "        print(out.shape)\n",
    "        assert out.shape == expected_shape, f\"Feature map shape {out.shape} does not match expected {expected_shape}\"\n",
    "\n",
    "    print(\"Forward pass test passed.\")\n",
    "    \n",
    "    \n",
    "test_mix_vision_transformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from models_torch.segformer import SegFormer, SegFormer_SHViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_segformer_b0():\n",
    "    input_shape = (3, 272, 272)  # Example input shape (channels, height, width)\n",
    "    num_classes = 10\n",
    "    model = SegFormer(model_type=\"B0\", input_shape=input_shape, num_classes=num_classes, use_resize=True)\n",
    "\n",
    "    # Create a dummy input tensor\n",
    "    dummy_input = torch.rand(1, *input_shape)  # Batch size of 1\n",
    "\n",
    "    # Perform a forward pass\n",
    "    output = model(dummy_input)\n",
    "\n",
    "    # Check the output shape\n",
    "    expected_output_shape = (1, num_classes, input_shape[1], input_shape[2])\n",
    "    print(output.shape, expected_output_shape)\n",
    "    assert output.shape == expected_output_shape, f\"Expected output shape {expected_output_shape}, but got {output.shape}\"\n",
    "\n",
    "    print(\"Test passed!\")\n",
    "\n",
    "# Run the test\n",
    "test_segformer_b0()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_segformer_shvit_b0_s4():\n",
    "    input_shape = (1, 272, 272)  # Example input shape (channels, height, width)\n",
    "    num_classes = 10\n",
    "    use_resize = False\n",
    "    model = SegFormer_SHViT(model_type=\"B0\", shvit_type=\"S4\", input_shape=input_shape, num_stages=3, num_classes=num_classes, use_resize=use_resize)\n",
    "\n",
    "    # Create a dummy input tensor\n",
    "    dummy_input = torch.rand(1, *input_shape)  # Batch size of 1\n",
    "\n",
    "    # Perform a forward pass\n",
    "    output = model(dummy_input)\n",
    "\n",
    "    # Check the output shape\n",
    "    factor = 1\n",
    "    if not use_resize:\n",
    "        factor = 16\n",
    "    expected_output_shape = (1, num_classes, input_shape[1]//factor, input_shape[2]//factor)\n",
    "    print(output.shape, expected_output_shape)\n",
    "    assert output.shape == expected_output_shape, f\"Expected output shape {expected_output_shape}, but got {output.shape}\"\n",
    "\n",
    "    print(\"Test passed!\")\n",
    "\n",
    "# Run the test\n",
    "test_segformer_shvit_b0_s4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_segformer_shvit_b0_s4():\n",
    "    input_shape = (1, 272, 272)  # Example input shape (channels, height, width)\n",
    "    num_classes = 10\n",
    "    use_resize = False\n",
    "    model = SegFormer_SHViT(model_type=\"B0\", shvit_type=\"S4\", input_shape=input_shape, num_stages=3, num_classes=num_classes, use_resize=use_resize)\n",
    "\n",
    "    # Create a dummy input tensor\n",
    "    dummy_input = torch.rand(1, *input_shape)  # Batch size of 1\n",
    "\n",
    "    # Perform a forward pass\n",
    "    output = model(dummy_input)\n",
    "\n",
    "    # Check the output shape\n",
    "    factor = 1\n",
    "    if not use_resize:\n",
    "        factor = 16\n",
    "    expected_output_shape = (1, num_classes, input_shape[1]//factor, input_shape[2]//factor)\n",
    "    print(output.shape, expected_output_shape)\n",
    "    assert output.shape == expected_output_shape, f\"Expected output shape {expected_output_shape}, but got {output.shape}\"\n",
    "\n",
    "    print(\"Test passed!\")\n",
    "\n",
    "# Run the test\n",
    "test_segformer_shvit_b0_s4()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch260",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
