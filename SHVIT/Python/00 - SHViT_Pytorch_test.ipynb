{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # https://github.com/maxw1489/Mask_RCNN (tensorflow 2.9.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cgd/anaconda3/envs/shvit/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from mmcv.cnn import ConvModule, DepthwiseSeparableConvModule\n",
    "from collections import OrderedDict\n",
    "\n",
    "# from mmseg.ops import resize # steht unten\n",
    "# from ..builder import HEADS\n",
    "# from .decode_head import BaseDecodeHead\n",
    "# from mmseg.models.utils import *\n",
    "# import attr\n",
    "\n",
    "from IPython import embed\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from timm.layers import SqueezeExcite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 256\n",
    "patch_size=16\n",
    "frozen_stages = 0\n",
    "in_chans = 3\n",
    "embed_dim = [224, 336, 448]\n",
    "partial_dim = [48, 72, 96] # partial_dim = r*embed_dim with r=1/4.67\n",
    "qk_dim = [16, 16, 16]\n",
    "depth = [4, 7, 6]\n",
    "types = [\"i\", \"s\", \"s\"]\n",
    "down_ops = [['subsample', 2], ['subsample', 2], ['']]\n",
    "pretrained = None\n",
    "distillation = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupNorm(torch.nn.GroupNorm):\n",
    "    \"\"\"\n",
    "    Group Normalization with 1 group.\n",
    "    Input: tensor in shape [B, C, H, W]\n",
    "    \"\"\"\n",
    "    def __init__(self, num_channels, **kwargs):\n",
    "        super().__init__(1, num_channels, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2d_BN(torch.nn.Sequential):  \n",
    "    def __init__(self, a, b, ks=1, stride=1, pad=0, dilation=1,\n",
    "                 groups=1, bn_weight_init=1):\n",
    "        super().__init__()\n",
    "        self.add_module('c', torch.nn.Conv2d(\n",
    "            a, b, ks, stride, pad, dilation, groups, bias=False))\n",
    "        self.add_module('bn', torch.nn.BatchNorm2d(b))\n",
    "        torch.nn.init.constant_(self.bn.weight, bn_weight_init)\n",
    "        torch.nn.init.constant_(self.bn.bias, 0)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def fuse(self):\n",
    "        c, bn = self._modules.values()\n",
    "        w = bn.weight / (bn.running_var + bn.eps)**0.5\n",
    "        w = c.weight * w[:, None, None, None]\n",
    "        b = bn.bias - bn.running_mean * bn.weight / \\\n",
    "            (bn.running_var + bn.eps)**0.5\n",
    "        m = torch.nn.Conv2d(w.size(1) * self.c.groups, w.size(\n",
    "            0), w.shape[2:], stride=self.c.stride, padding=self.c.padding, dilation=self.c.dilation, groups=self.c.groups)\n",
    "        m.weight.data.copy_(w)\n",
    "        m.bias.data.copy_(b)\n",
    "        return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BN_Linear(torch.nn.Sequential):\n",
    "    def __init__(self, a, b, bias=True, std=0.02):\n",
    "        super().__init__()\n",
    "        self.add_module('bn', torch.nn.BatchNorm1d(a))\n",
    "        self.add_module('l', torch.nn.Linear(a, b, bias=bias))\n",
    "        trunc_normal_(self.l.weight, std=std)\n",
    "        if bias:\n",
    "            torch.nn.init.constant_(self.l.bias, 0)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def fuse(self):\n",
    "        bn, l = self._modules.values()\n",
    "        w = bn.weight / (bn.running_var + bn.eps)**0.5\n",
    "        b = bn.bias - self.bn.running_mean * \\\n",
    "            self.bn.weight / (bn.running_var + bn.eps)**0.5\n",
    "        w = l.weight * w[None, :]\n",
    "        if l.bias is None:\n",
    "            b = b @ self.l.weight.T\n",
    "        else:\n",
    "            b = (l.weight @ b[:, None]).view(-1) + self.l.bias\n",
    "        m = torch.nn.Linear(w.size(1), w.size(0))\n",
    "        m.weight.data.copy_(w)\n",
    "        m.bias.data.copy_(b)\n",
    "        return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchMerging(torch.nn.Module):\n",
    "    def __init__(self, dim, out_dim):\n",
    "        super().__init__()\n",
    "        hid_dim = int(dim * 4)\n",
    "        self.conv1 = Conv2d_BN(dim, hid_dim, 1, 1, 0)\n",
    "        self.act = torch.nn.ReLU()\n",
    "        self.conv2 = Conv2d_BN(hid_dim, hid_dim, 3, 2, 1, groups=hid_dim)\n",
    "        self.se = SqueezeExcite(hid_dim, .25)\n",
    "        self.conv3 = Conv2d_BN(hid_dim, out_dim, 1, 1, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"### PatchMerging ###\")\n",
    "        print(x.shape)\n",
    "        x = self.conv3(self.se(self.act(self.conv2(self.act(self.conv1(x))))))\n",
    "        print(x.shape)\n",
    "        print(\"===\"*10)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(torch.nn.Module):\n",
    "    def __init__(self, m, drop=0.):\n",
    "        super().__init__()\n",
    "        self.m = m\n",
    "        self.drop = drop\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training and self.drop > 0:\n",
    "            return x + self.m(x) * torch.rand(x.size(0), 1, 1, 1,\n",
    "                                              device=x.device).ge_(self.drop).div(1 - self.drop).detach()\n",
    "        else:\n",
    "            return x + self.m(x)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def fuse(self):\n",
    "        if isinstance(self.m, Conv2d_BN):\n",
    "            m = self.m.fuse()\n",
    "            assert(m.groups == m.in_channels)\n",
    "            identity = torch.ones(m.weight.shape[0], m.weight.shape[1], 1, 1)\n",
    "            identity = torch.nn.functional.pad(identity, [1,1,1,1])\n",
    "            m.weight += identity.to(m.weight.device)\n",
    "            return m\n",
    "        else:\n",
    "            return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(torch.nn.Module):\n",
    "    def __init__(self, ed, h):\n",
    "        super().__init__()\n",
    "        self.pw1 = Conv2d_BN(ed, h)\n",
    "        self.act = torch.nn.ReLU()\n",
    "        self.pw2 = Conv2d_BN(h, ed, bn_weight_init=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pw2(self.act(self.pw1(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SHSA(torch.nn.Module):\n",
    "    \"\"\"Single-Head Self-Attention\"\"\"\n",
    "    def __init__(self, dim, qk_dim, pdim):\n",
    "        super().__init__()\n",
    "        self.scale = qk_dim ** -0.5\n",
    "        self.qk_dim = qk_dim\n",
    "        self.dim = dim\n",
    "        self.pdim = pdim\n",
    "\n",
    "        self.pre_norm = GroupNorm(pdim)\n",
    "\n",
    "        self.qkv = Conv2d_BN(pdim, qk_dim * 2 + pdim)\n",
    "        self.proj = torch.nn.Sequential(torch.nn.ReLU(), Conv2d_BN(\n",
    "            dim, dim, bn_weight_init = 0))\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x1, x2 = torch.split(x, [self.pdim, self.dim - self.pdim], dim = 1)\n",
    "        x1 = self.pre_norm(x1)\n",
    "        qkv = self.qkv(x1)\n",
    "        q, k, v = qkv.split([self.qk_dim, self.qk_dim, self.pdim], dim = 1)\n",
    "        q, k, v = q.flatten(2), k.flatten(2), v.flatten(2)\n",
    "        \n",
    "        attn = (q.transpose(-2, -1) @ k) * self.scale\n",
    "        attn = attn.softmax(dim = -1)\n",
    "        x1 = (v @ attn.transpose(-2, -1)).reshape(B, self.pdim, H, W)\n",
    "        x = self.proj(torch.cat([x1, x2], dim = 1))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(torch.nn.Module):\n",
    "    def __init__(self, dim, qk_dim, pdim, type):\n",
    "        super().__init__()\n",
    "        if type == \"s\":    # for later stages\n",
    "            self.conv = Residual(Conv2d_BN(dim, dim, 3, 1, 1, groups = dim, bn_weight_init = 0))\n",
    "            self.mixer = Residual(SHSA(dim, qk_dim, pdim))\n",
    "            self.ffn = Residual(FFN(dim, int(dim * 2)))\n",
    "        elif type == \"i\":   # for early stages\n",
    "            self.conv = Residual(Conv2d_BN(dim, dim, 3, 1, 1, groups = dim, bn_weight_init = 0))\n",
    "            self.mixer = torch.nn.Identity()\n",
    "            self.ffn = Residual(FFN(dim, int(dim * 2)))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.ffn(self.mixer(self.conv(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npatch_embed = torch.nn.Sequential(Conv2d_BN(in_chans, embed_dim[0] // 8, 3, 2, 1), torch.nn.ReLU(),\\n                           Conv2d_BN(embed_dim[0] // 8, embed_dim[0] // 4, 3, 2, 1), torch.nn.ReLU(),\\n                           Conv2d_BN(embed_dim[0] // 4, embed_dim[0] // 2, 3, 2, 1), torch.nn.ReLU(),\\n                           Conv2d_BN(embed_dim[0] // 2, embed_dim[0], 3, 2, 1)\\n                           )\\n\\nx = patch_embed(input_image)\\nprint(\"patch_embed: \", x.shape)\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input_image = torch.randn(1, 3, 512, 512)\n",
    "'''\n",
    "patch_embed = torch.nn.Sequential(Conv2d_BN(in_chans, embed_dim[0] // 8, 3, 2, 1), torch.nn.ReLU(),\n",
    "                           Conv2d_BN(embed_dim[0] // 8, embed_dim[0] // 4, 3, 2, 1), torch.nn.ReLU(),\n",
    "                           Conv2d_BN(embed_dim[0] // 4, embed_dim[0] // 2, 3, 2, 1), torch.nn.ReLU(),\n",
    "                           Conv2d_BN(embed_dim[0] // 2, embed_dim[0], 3, 2, 1)\n",
    "                           )\n",
    "\n",
    "x = patch_embed(input_image)\n",
    "print(\"patch_embed: \", x.shape)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nblocks1 = []\\nblocks2 = []\\nblocks3 = []\\nouts = []\\n\\n\\nfor i, (ed, kd, pd, dpth, do, t) in enumerate(zip(embed_dim, qk_dim, partial_dim, depth, down_ops, types)):\\n    print (i, ed, kd, pd, dpth, do, t)\\n    print(\"dpth: \", dpth)\\n    for d in range(dpth):\\n        eval(\\'blocks\\' + str(i+1)).append(BasicBlock(ed, kd, pd, t))\\n    if do[0] == \\'subsample\\':\\n                # Build SHViT downsample block\\n                #(\\'Subsample\\' stride)\\n                blk = eval(\\'blocks\\' + str(i+1)) # 2\\n                blk.append(torch.nn.Sequential(Residual(Conv2d_BN(embed_dim[i], embed_dim[i], 3, 1, 1, groups=embed_dim[i])),\\n                                    Residual(FFN(embed_dim[i], int(embed_dim[i] * 2))),))\\n                blk.append(PatchMerging(*embed_dim[i:i + 2]))\\n                \\n                blk.append(torch.nn.Sequential(Residual(Conv2d_BN(embed_dim[i + 1], embed_dim[i + 1], 3, 1, 1, groups=embed_dim[i + 1])),\\n                                    Residual(FFN(embed_dim[i + 1], int(embed_dim[i + 1] * 2))),))\\n\\nprint(eval(\\'blocks1\\'))\\n\\nprint(len(blocks1),len(blocks2),len(blocks3)) # 4, 7+3, 6+3\\n\\nblocks1 = torch.nn.Sequential(*blocks1)\\nblocks2 = torch.nn.Sequential(*blocks2)\\nblocks3 = torch.nn.Sequential(*blocks3)\\n\\nprint()\\n\\nprint(\"Stage 1: \")\\nprint(\"block1 in : \", x.shape)\\nprint(\"block1 in : \", x.shape)\\nx = blocks1(x)\\nouts.append(x)\\nprint(\"block1 out: \", x.shape)\\n\\nprint()\\nprint(\"Stage 2: \")\\nprint(\"block2 in : \", x.shape)\\nx = blocks2(x)\\nouts.append(x)\\nprint(\"block2 out: \", x.shape)\\n\\nprint()\\nprint(\"Stage 3: \")\\nprint(\"block3 in : \", x.shape)\\nx = blocks3(x)\\nouts.append(x)\\nprint(\"block3 out: \", x.shape)\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "blocks1 = []\n",
    "blocks2 = []\n",
    "blocks3 = []\n",
    "outs = []\n",
    "\n",
    "\n",
    "for i, (ed, kd, pd, dpth, do, t) in enumerate(zip(embed_dim, qk_dim, partial_dim, depth, down_ops, types)):\n",
    "    print (i, ed, kd, pd, dpth, do, t)\n",
    "    print(\"dpth: \", dpth)\n",
    "    for d in range(dpth):\n",
    "        eval('blocks' + str(i+1)).append(BasicBlock(ed, kd, pd, t))\n",
    "    if do[0] == 'subsample':\n",
    "                # Build SHViT downsample block\n",
    "                #('Subsample' stride)\n",
    "                blk = eval('blocks' + str(i+1)) # 2\n",
    "                blk.append(torch.nn.Sequential(Residual(Conv2d_BN(embed_dim[i], embed_dim[i], 3, 1, 1, groups=embed_dim[i])),\n",
    "                                    Residual(FFN(embed_dim[i], int(embed_dim[i] * 2))),))\n",
    "                blk.append(PatchMerging(*embed_dim[i:i + 2]))\n",
    "                \n",
    "                blk.append(torch.nn.Sequential(Residual(Conv2d_BN(embed_dim[i + 1], embed_dim[i + 1], 3, 1, 1, groups=embed_dim[i + 1])),\n",
    "                                    Residual(FFN(embed_dim[i + 1], int(embed_dim[i + 1] * 2))),))\n",
    "\n",
    "print(eval('blocks1'))\n",
    "\n",
    "print(len(blocks1),len(blocks2),len(blocks3)) # 4, 7+3, 6+3\n",
    "\n",
    "blocks1 = torch.nn.Sequential(*blocks1)\n",
    "blocks2 = torch.nn.Sequential(*blocks2)\n",
    "blocks3 = torch.nn.Sequential(*blocks3)\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Stage 1: \")\n",
    "print(\"block1 in : \", x.shape)\n",
    "print(\"block1 in : \", x.shape)\n",
    "x = blocks1(x)\n",
    "outs.append(x)\n",
    "print(\"block1 out: \", x.shape)\n",
    "\n",
    "print()\n",
    "print(\"Stage 2: \")\n",
    "print(\"block2 in : \", x.shape)\n",
    "x = blocks2(x)\n",
    "outs.append(x)\n",
    "print(\"block2 out: \", x.shape)\n",
    "\n",
    "print()\n",
    "print(\"Stage 3: \")\n",
    "print(\"block3 in : \", x.shape)\n",
    "x = blocks3(x)\n",
    "outs.append(x)\n",
    "print(\"block3 out: \", x.shape)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n   def __init__(self,\\n                 in_chans=3,\\n                 num_classes=1000,\\n                 embed_dim= [224, 336, 448], # [128, 256, 384] SHVIT_\\n                 partial_dim = [32, 64, 96],\\n                 qk_dim=[16, 16, 16],\\n                 depth=[1, 2, 3],\\n                 types = [\"s\", \"s\", \"s\"],   \\t\\n                 down_ops=[[\\'subsample\\', 2], [\\'subsample\\', 2], [\\'\\']],\\n                 distillation=False,):\\n        super().__init__()\\n\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "   def __init__(self,\n",
    "                 in_chans=3,\n",
    "                 num_classes=1000,\n",
    "                 embed_dim= [224, 336, 448], # [128, 256, 384] SHVIT_\n",
    "                 partial_dim = [32, 64, 96],\n",
    "                 qk_dim=[16, 16, 16],\n",
    "                 depth=[1, 2, 3],\n",
    "                 types = [\"s\", \"s\", \"s\"],   \t\n",
    "                 down_ops=[['subsample', 2], ['subsample', 2], ['']],\n",
    "                 distillation=False,):\n",
    "        super().__init__()\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 256\n",
    "patch_size=16\n",
    "frozen_stages = 0\n",
    "in_chans = 3\n",
    "embed_dim = [224, 336, 448]\n",
    "partial_dim = [48, 72, 96] # partial_dim = r*embed_dim with r=1/4.67\n",
    "qk_dim = [16, 16, 16]\n",
    "depth = [4, 7, 6]\n",
    "types = [\"i\", \"s\", \"s\"]\n",
    "down_ops = [['subsample', 2], ['subsample', 2], ['']]\n",
    "pretrained = None\n",
    "distillation = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SHViT(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_chans=3,\n",
    "                 num_classes=1000,\n",
    "                 embed_dim = [224, 336, 448], # [128, 256, 384] SHVIT_\n",
    "                 partial_dim = [48, 72, 96],\n",
    "                 qk_dim=[16, 16, 16],\n",
    "                 depth = [4, 7, 6],\n",
    "                 types = [\"i\", \"s\", \"s\"],   \t\n",
    "                 down_ops=[['subsample', 2], ['subsample', 2], ['']],\n",
    "                 distillation=False,):\n",
    "        super().__init__()\n",
    "\n",
    "        # Patch embedding\n",
    "        self.patch_embed = torch.nn.Sequential(Conv2d_BN(in_chans, embed_dim[0] // 8, 3, 2, 1), torch.nn.ReLU(),\n",
    "                           Conv2d_BN(embed_dim[0] // 8, embed_dim[0] // 4, 3, 2, 1), torch.nn.ReLU(),\n",
    "                           Conv2d_BN(embed_dim[0] // 4, embed_dim[0] // 2, 3, 2, 1), torch.nn.ReLU(),\n",
    "                           Conv2d_BN(embed_dim[0] // 2, embed_dim[0], 3, 2, 1))\n",
    "\n",
    "        self.blocks1 = []\n",
    "        self.blocks2 = []\n",
    "        self.blocks3 = []\n",
    "\n",
    "        # Build SHViT blocks\n",
    "        for i, (ed, kd, pd, dpth, do, t) in enumerate(\n",
    "                zip(embed_dim, qk_dim, partial_dim, depth, down_ops, types)):\n",
    "            for d in range(dpth):\n",
    "                eval('self.blocks' + str(i+1)).append(BasicBlock(ed, kd, pd, t))\n",
    "            if do[0] == 'subsample':\n",
    "                # Build SHViT downsample block\n",
    "                #('Subsample' stride)\n",
    "                blk = eval('self.blocks' + str(i+2)) # mÃ¼ssete 1 sein # war 2\n",
    "                blk.append(torch.nn.Sequential(Residual(Conv2d_BN(embed_dim[i], embed_dim[i], 3, 1, 1, groups=embed_dim[i])),\n",
    "                                    Residual(FFN(embed_dim[i], int(embed_dim[i] * 2))),))\n",
    "                blk.append(PatchMerging(*embed_dim[i:i + 2]))\n",
    "                \n",
    "                blk.append(torch.nn.Sequential(Residual(Conv2d_BN(embed_dim[i + 1], embed_dim[i + 1], 3, 1, 1, groups=embed_dim[i + 1])),\n",
    "                                    Residual(FFN(embed_dim[i + 1], int(embed_dim[i + 1] * 2))),))\n",
    "        self.blocks1 = torch.nn.Sequential(*self.blocks1)\n",
    "        \n",
    "        self.blocks2 = torch.nn.Sequential(*self.blocks2)\n",
    "        self.blocks3 = torch.nn.Sequential(*self.blocks3)\n",
    "        \n",
    "        '''\n",
    "        # Classification head\n",
    "        self.head = BN_Linear(embed_dim[-1], num_classes) if num_classes > 0 else torch.nn.Identity()\n",
    "        self.distillation = distillation\n",
    "        if distillation:\n",
    "            self.head_dist = BN_Linear(embed_dim[-1], num_classes) if num_classes > 0 else torch.nn.Identity()\n",
    "        '''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        print(\"x: \", x.shape)\n",
    "        x = self.blocks1(x)\n",
    "        print(\"block1 out: \", x.shape)\n",
    "        x = self.blocks2(x)\n",
    "        print(\"block2 out: \", x.shape)\n",
    "        x = self.blocks3(x)\n",
    "        print(\"block3 out: \", x.shape)\n",
    "\n",
    "        '''\n",
    "        x = torch.nn.functional.adaptive_avg_pool2d(x, 1).flatten(1)\n",
    "        if self.distillation:\n",
    "            x = self.head(x), self.head_dist(x)\n",
    "            if not self.training:\n",
    "                x = (x[0] + x[1]) / 2\n",
    "        else:\n",
    "            x = self.head(x)\n",
    "        '''\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ninput_image = torch.randn(1, 3, 512, 512)\\nin_channels = SHViT().forward(input_image)  \\nc3_in_channels = in_channels\\n\\nprint(\"input_image shape: \", input_image.shape)\\nin_channels.shape\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "input_image = torch.randn(1, 3, 512, 512)\n",
    "in_channels = SHViT().forward(input_image)  \n",
    "c3_in_channels = in_channels\n",
    "\n",
    "print(\"input_image shape: \", input_image.shape)\n",
    "in_channels.shape\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================== SegFormer - Head =========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "shvit_bibi = SHViT()\n",
    "# shvit_bibi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize(input,\n",
    "           size=None,\n",
    "           scale_factor=None,\n",
    "           mode='nearest',\n",
    "           align_corners=None,\n",
    "           warning=True):\n",
    "    if warning:\n",
    "        if size is not None and align_corners:\n",
    "            input_h, input_w = tuple(int(x) for x in input.shape[2:])\n",
    "            output_h, output_w = tuple(int(x) for x in size)\n",
    "            if output_h > input_h or output_w > output_h:\n",
    "                if ((output_h > 1 and output_w > 1 and input_h > 1\n",
    "                     and input_w > 1) and (output_h - 1) % (input_h - 1)\n",
    "                        and (output_w - 1) % (input_w - 1)):\n",
    "                    warnings.warn(\n",
    "                        f'When align_corners={align_corners}, '\n",
    "                        'the output would more aligned if '\n",
    "                        f'input size {(input_h, input_w)} is `x+1` and '\n",
    "                        f'out size {(output_h, output_w)} is `nx+1`')\n",
    "    if isinstance(size, torch.Size):\n",
    "        size = tuple(int(x) for x in size)\n",
    "    return F.interpolate(input, size, scale_factor, mode, align_corners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Linear Embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=2048, embed_dim=768): \n",
    "        \"\"\"\n",
    "        Args:\n",
    "            c3: input_dim\n",
    "            c_out = embed_dim\n",
    "            F_i = x (computed tensor from SHVIT)\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(input_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        x = self.proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init function \n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "input_image = torch.randn(1, 3, 512, 512)#.to(device)\n",
    "embedding_dim = 448 # 768\n",
    "num_classes = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OKKK 128.0 128.0\n",
      "x:  torch.Size([1, 224, 32, 32])\n",
      "block1 out:  torch.Size([1, 224, 32, 32])\n",
      "### PatchMerging ###\n",
      "torch.Size([1, 224, 32, 32])\n",
      "torch.Size([1, 336, 16, 16])\n",
      "==============================\n",
      "block2 out:  torch.Size([1, 336, 16, 16])\n",
      "### PatchMerging ###\n",
      "torch.Size([1, 336, 16, 16])\n",
      "torch.Size([1, 448, 8, 8])\n",
      "==============================\n",
      "block3 out:  torch.Size([1, 448, 8, 8])\n",
      "=======================================================================================\n",
      "torch.Size([1, 448, 8, 8])\n",
      "torch.Size([1, 448, 128, 128])\n",
      "SSSSSSSSShape:  torch.Size([1, 15, 128, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 15, 128, 128])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embedding_dim = 448 # 768\n",
    "class SegFormerHead(nn.Module):\n",
    "    def __init__(self, num_classes=150, embedding_dim = 448, feature_strides=None, channels=None,  in_channels=448,  dropout_ratio=0.1, **kwargs):\n",
    "        super(SegFormerHead, self).__init__(**kwargs)\n",
    "        self.in_channels = in_channels\n",
    "        c1_in_channels = self.in_channels\n",
    "        self.num_classes = num_classes\n",
    "        self.feature_strides = feature_strides  \n",
    "\n",
    "        \n",
    "\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        if self.dropout_ratio > 0:\n",
    "            self.dropout = nn.Dropout2d(dropout_ratio)\n",
    "        else:\n",
    "            self.dropout = None\n",
    "\n",
    "\n",
    "        self.linear_c1 = MLP(input_dim=c1_in_channels, embed_dim=embedding_dim)\n",
    "        self.linear_fuse = ConvModule(\n",
    "            in_channels=embedding_dim, # 4C --> C --> embedding_dim\n",
    "            out_channels=embedding_dim,\n",
    "            kernel_size=1,\n",
    "            # norm_cfg=dict(type='SyncBN', requires_grad=True)\n",
    "        )\n",
    "        self.linear_pred = nn.Conv2d(embedding_dim, self.num_classes, kernel_size=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        _, _, h_shvit, w_shvit = input_image.size()\n",
    "        print(\"OKKK\", h_shvit/4,w_shvit/4)\n",
    "\n",
    "\n",
    "        x = shvit_bibi(inputs)#.to(device)\n",
    "        c1 = x\n",
    "        n, _, h, w = c1.shape\n",
    "\n",
    "        print(\"=======================================================================================\")\n",
    "        print(c1.shape)\n",
    "        _c1 = self.linear_c1(c1).permute(0,2,1).reshape(n, -1, c1.shape[2], c1.shape[3])\n",
    "        # h_shvit/4,w_shvit/4\n",
    "        _c1 = resize(_c1, size=(128, 128),mode='bilinear',align_corners=False)\n",
    "        print(_c1.shape)\n",
    "\n",
    "        _c = self.linear_fuse(torch.cat([_c1.to('cpu')], dim=1))\n",
    "\n",
    "        \n",
    "        \n",
    "        x = self.dropout(_c)\n",
    "        x = self.linear_pred(x)\n",
    "        print(\"SSSSSSSSShape: \", x.shape)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "tmp_result = SegFormerHead(num_classes=15, in_channels=448).forward(input_image) #.to(device)\n",
    "tmp_result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################## FERTIG!! ##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_dim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dim \u001b[38;5;241m=\u001b[39m \u001b[43minput_dim\u001b[49m\n\u001b[1;32m      2\u001b[0m hid_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(dim \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m      3\u001b[0m conv1 \u001b[38;5;241m=\u001b[39m Conv2d_BN(dim, hid_dim, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'input_dim' is not defined"
     ]
    }
   ],
   "source": [
    "dim = input_dim\n",
    "hid_dim = int(dim * 4)\n",
    "conv1 = Conv2d_BN(dim, hid_dim, 1, 1, 0)\n",
    "conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "#  x = self._transform_inputs(inputs) \n",
    "\n",
    "c3 = x\n",
    "_c3 = linear_c3(c3).permute(0,2,1).reshape(n, -1, c3.shape[2], c3.shape[3])\n",
    "_c3 = resize(_c3, size=c1.size()[2:],mode='bilinear',align_corners=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegFormerHead(SHViT):\n",
    "    \"\"\"\n",
    "    SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers\n",
    "    https://github.com/NVlabs/SegFormer/blob/master/local_configs/segformer/B0/segformer.b0.512x512.ade.160k.py#L18 - wegen feature_strides\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_strides, **kwargs):\n",
    "        super(SegFormerHead, self).__init__(input_transform='multiple_select', **kwargs)\n",
    "        assert len(feature_strides) == len(self.in_channels)\n",
    "        assert min(feature_strides) == feature_strides[0]\n",
    "        self.feature_strides = feature_strides\n",
    "\n",
    "        c1_in_channels, c2_in_channels, c3_in_channels, c4_in_channels = self.in_channels\n",
    "\n",
    "        decoder_params = kwargs['decoder_params']\n",
    "        embedding_dim = decoder_params['embed_dim']\n",
    "\n",
    "        # self.linear_c4 = MLP(input_dim=c4_in_channels, embed_dim=embedding_dim)\n",
    "        self.linear_c3 = MLP(input_dim=c3_in_channels, embed_dim=embedding_dim)\n",
    "        # self.linear_c2 = MLP(input_dim=c2_in_channels, embed_dim=embedding_dim)\n",
    "        # self.linear_c1 = MLP(input_dim=c1_in_channels, embed_dim=embedding_dim)\n",
    "\n",
    "        self.linear_fuse = ConvModule(\n",
    "            in_channels=embedding_dim*4,\n",
    "            out_channels=embedding_dim,\n",
    "            kernel_size=1,\n",
    "            norm_cfg=dict(type='SyncBN', requires_grad=True)\n",
    "        )\n",
    "\n",
    "        self.linear_pred = nn.Conv2d(embedding_dim, self.num_classes, kernel_size=1)\n",
    "\n",
    "\n",
    "\n",
    "        def forward(self, inputs):\n",
    "            x = self._transform_inputs(inputs)  # len=4, 1/4,1/8,1/16,1/32\n",
    "            # c1, c2, c3, c4 = x\n",
    "\n",
    "            ############## MLP decoder on C1-C4 ###########\n",
    "            n, _, h, w = c4.shape\n",
    "\n",
    "            _c4 = self.linear_c4(c4).permute(0,2,1).reshape(n, -1, c4.shape[2], c4.shape[3])\n",
    "            _c4 = resize(_c4, size=c1.size()[2:],mode='bilinear',align_corners=False)\n",
    "\n",
    "            # _c3 = self.linear_c3(c3).permute(0,2,1).reshape(n, -1, c3.shape[2], c3.shape[3])\n",
    "            # _c3 = resize(_c3, size=c1.size()[2:],mode='bilinear',align_corners=False)\n",
    "\n",
    "            # _c2 = self.linear_c2(c2).permute(0,2,1).reshape(n, -1, c2.shape[2], c2.shape[3])\n",
    "            # _c2 = resize(_c2, size=c1.size()[2:],mode='bilinear',align_corners=False)\n",
    "\n",
    "            # _c1 = self.linear_c1(c1).permute(0,2,1).reshape(n, -1, c1.shape[2], c1.shape[3])\n",
    "\n",
    "            # _c = self.linear_fuse(torch.cat([_c4, _c3, _c2, _c1], dim=1))\n",
    "\n",
    "            _c = self.linear_fuse(torch.cat([_c4], dim=1))\n",
    "            x = self.dropout(_c)\n",
    "            x = self.linear_pred(x)\n",
    "\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegFormerHead(SHViT):\n",
    "    \"\"\"\n",
    "    SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_strides, **kwargs):\n",
    "        super(SegFormerHead, self).__init__(input_transform='multiple_select', **kwargs)\n",
    "        assert len(feature_strides) == len(self.in_channels)\n",
    "        assert min(feature_strides) == feature_strides[0]\n",
    "        self.feature_strides = feature_strides\n",
    "\n",
    "        c1_in_channels, c2_in_channels, c3_in_channels, c4_in_channels = self.in_channels\n",
    "\n",
    "        decoder_params = kwargs['decoder_params']\n",
    "        embedding_dim = decoder_params['embed_dim']\n",
    "\n",
    "        self.linear_c4 = MLP(input_dim=c4_in_channels, embed_dim=embedding_dim)\n",
    "        # self.linear_c3 = MLP(input_dim=c3_in_channels, embed_dim=embedding_dim)\n",
    "        # self.linear_c2 = MLP(input_dim=c2_in_channels, embed_dim=embedding_dim)\n",
    "        # self.linear_c1 = MLP(input_dim=c1_in_channels, embed_dim=embedding_dim)\n",
    "\n",
    "        self.linear_fuse = ConvModule(\n",
    "            in_channels=embedding_dim*4,\n",
    "            out_channels=embedding_dim,\n",
    "            kernel_size=1,\n",
    "            norm_cfg=dict(type='SyncBN', requires_grad=True)\n",
    "        )\n",
    "\n",
    "        self.linear_pred = nn.Conv2d(embedding_dim, self.num_classes, kernel_size=1)\n",
    "\n",
    "\n",
    "\n",
    "        def forward(self, inputs):\n",
    "            x = self._transform_inputs(inputs)  # len=4, 1/4,1/8,1/16,1/32\n",
    "            # c1, c2, c3, c4 = x\n",
    "            c4 = x\n",
    "\n",
    "            ############## MLP decoder on C1-C4 ###########\n",
    "            n, _, h, w = c4.shape\n",
    "\n",
    "            _c4 = self.linear_c4(c4).permute(0,2,1).reshape(n, -1, c4.shape[2], c4.shape[3])\n",
    "            _c4 = resize(_c4, size=c1.size()[2:],mode='bilinear',align_corners=False)\n",
    "\n",
    "            # _c3 = self.linear_c3(c3).permute(0,2,1).reshape(n, -1, c3.shape[2], c3.shape[3])\n",
    "            # _c3 = resize(_c3, size=c1.size()[2:],mode='bilinear',align_corners=False)\n",
    "\n",
    "            # _c2 = self.linear_c2(c2).permute(0,2,1).reshape(n, -1, c2.shape[2], c2.shape[3])\n",
    "            # _c2 = resize(_c2, size=c1.size()[2:],mode='bilinear',align_corners=False)\n",
    "\n",
    "            # _c1 = self.linear_c1(c1).permute(0,2,1).reshape(n, -1, c1.shape[2], c1.shape[3])\n",
    "\n",
    "            # _c = self.linear_fuse(torch.cat([_c4, _c3, _c2, _c1], dim=1))\n",
    "\n",
    "            _c = self.linear_fuse(torch.cat([_c4], dim=1))\n",
    "            x = self.dropout(_c)\n",
    "            x = self.linear_pred(x)\n",
    "\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "512//16, 512//32, 512//64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes = [tensor.shape for tensor in outs]\n",
    "print(shapes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shvit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
