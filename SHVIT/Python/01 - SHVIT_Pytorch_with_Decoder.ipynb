{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cgd/anaconda3/envs/shvit/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from timm.layers import SqueezeExcite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupNorm(torch.nn.GroupNorm):\n",
    "    \"\"\"\n",
    "    Group Normalization with 1 group.\n",
    "    Input: tensor in shape [B, C, H, W]\n",
    "    \"\"\"\n",
    "    def __init__(self, num_channels, **kwargs):\n",
    "        super().__init__(1, num_channels, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2d_BN(torch.nn.Sequential):  \n",
    "    def __init__(self, a, b, ks=1, stride=1, pad=0, dilation=1,\n",
    "                 groups=1, bn_weight_init=1):\n",
    "        super().__init__()\n",
    "        self.add_module('c', torch.nn.Conv2d(\n",
    "            a, b, ks, stride, pad, dilation, groups, bias=False))\n",
    "        self.add_module('bn', torch.nn.BatchNorm2d(b))\n",
    "        torch.nn.init.constant_(self.bn.weight, bn_weight_init)\n",
    "        torch.nn.init.constant_(self.bn.bias, 0)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def fuse(self):\n",
    "        c, bn = self._modules.values()\n",
    "        w = bn.weight / (bn.running_var + bn.eps)**0.5\n",
    "        w = c.weight * w[:, None, None, None]\n",
    "        b = bn.bias - bn.running_mean * bn.weight / \\\n",
    "            (bn.running_var + bn.eps)**0.5\n",
    "        m = torch.nn.Conv2d(w.size(1) * self.c.groups, w.size(\n",
    "            0), w.shape[2:], stride=self.c.stride, padding=self.c.padding, dilation=self.c.dilation, groups=self.c.groups)\n",
    "        m.weight.data.copy_(w)\n",
    "        m.bias.data.copy_(b)\n",
    "        return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BN_Linear(torch.nn.Sequential):\n",
    "    def __init__(self, a, b, bias=True, std=0.02):\n",
    "        super().__init__()\n",
    "        self.add_module('bn', torch.nn.BatchNorm1d(a))\n",
    "        self.add_module('l', torch.nn.Linear(a, b, bias=bias))\n",
    "        trunc_normal_(self.l.weight, std=std)\n",
    "        if bias:\n",
    "            torch.nn.init.constant_(self.l.bias, 0)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def fuse(self):\n",
    "        bn, l = self._modules.values()\n",
    "        w = bn.weight / (bn.running_var + bn.eps)**0.5\n",
    "        b = bn.bias - self.bn.running_mean * \\\n",
    "            self.bn.weight / (bn.running_var + bn.eps)**0.5\n",
    "        w = l.weight * w[None, :]\n",
    "        if l.bias is None:\n",
    "            b = b @ self.l.weight.T\n",
    "        else:\n",
    "            b = (l.weight @ b[:, None]).view(-1) + self.l.bias\n",
    "        m = torch.nn.Linear(w.size(1), w.size(0))\n",
    "        m.weight.data.copy_(w)\n",
    "        m.bias.data.copy_(b)\n",
    "        return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchMerging(torch.nn.Module):\n",
    "    def __init__(self, dim, out_dim):\n",
    "        super().__init__()\n",
    "        hid_dim = int(dim * 4)\n",
    "        self.conv1 = Conv2d_BN(dim, hid_dim, 1, 1, 0)\n",
    "        self.act = torch.nn.ReLU()\n",
    "        self.conv2 = Conv2d_BN(hid_dim, hid_dim, 3, 2, 1, groups=hid_dim)\n",
    "        self.se = SqueezeExcite(hid_dim, .25)\n",
    "        self.conv3 = Conv2d_BN(hid_dim, out_dim, 1, 1, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"### PatchMerging ###\")\n",
    "        print(x.shape)\n",
    "        x = self.conv3(self.se(self.act(self.conv2(self.act(self.conv1(x))))))\n",
    "        print(x.shape)\n",
    "        print(\"===\"*10)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(torch.nn.Module):\n",
    "    def __init__(self, m, drop=0.):\n",
    "        super().__init__()\n",
    "        self.m = m\n",
    "        self.drop = drop\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training and self.drop > 0:\n",
    "            return x + self.m(x) * torch.rand(x.size(0), 1, 1, 1,\n",
    "                                              device=x.device).ge_(self.drop).div(1 - self.drop).detach()\n",
    "        else:\n",
    "            return x + self.m(x)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def fuse(self):\n",
    "        if isinstance(self.m, Conv2d_BN):\n",
    "            m = self.m.fuse()\n",
    "            assert(m.groups == m.in_channels)\n",
    "            identity = torch.ones(m.weight.shape[0], m.weight.shape[1], 1, 1)\n",
    "            identity = torch.nn.functional.pad(identity, [1,1,1,1])\n",
    "            m.weight += identity.to(m.weight.device)\n",
    "            return m\n",
    "        else:\n",
    "            return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(torch.nn.Module):\n",
    "    def __init__(self, ed, h):\n",
    "        super().__init__()\n",
    "        self.pw1 = Conv2d_BN(ed, h)\n",
    "        self.act = torch.nn.ReLU()\n",
    "        self.pw2 = Conv2d_BN(h, ed, bn_weight_init=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pw2(self.act(self.pw1(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SHSA(torch.nn.Module):\n",
    "    \"\"\"Single-Head Self-Attention\"\"\"\n",
    "    def __init__(self, dim, qk_dim, pdim):\n",
    "        super().__init__()\n",
    "        self.scale = qk_dim ** -0.5\n",
    "        self.qk_dim = qk_dim\n",
    "        self.dim = dim\n",
    "        self.pdim = pdim\n",
    "\n",
    "        self.pre_norm = GroupNorm(pdim)\n",
    "\n",
    "        self.qkv = Conv2d_BN(pdim, qk_dim * 2 + pdim)\n",
    "        self.proj = torch.nn.Sequential(torch.nn.ReLU(), Conv2d_BN(\n",
    "            dim, dim, bn_weight_init = 0))\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x1, x2 = torch.split(x, [self.pdim, self.dim - self.pdim], dim = 1)\n",
    "        x1 = self.pre_norm(x1)\n",
    "        qkv = self.qkv(x1)\n",
    "        q, k, v = qkv.split([self.qk_dim, self.qk_dim, self.pdim], dim = 1)\n",
    "        q, k, v = q.flatten(2), k.flatten(2), v.flatten(2)\n",
    "        \n",
    "        attn = (q.transpose(-2, -1) @ k) * self.scale\n",
    "        attn = attn.softmax(dim = -1)\n",
    "        x1 = (v @ attn.transpose(-2, -1)).reshape(B, self.pdim, H, W)\n",
    "        x = self.proj(torch.cat([x1, x2], dim = 1))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(torch.nn.Module):\n",
    "    def __init__(self, dim, qk_dim, pdim, type):\n",
    "        super().__init__()\n",
    "        if type == \"s\":    # for later stages\n",
    "            self.conv = Residual(Conv2d_BN(dim, dim, 3, 1, 1, groups = dim, bn_weight_init = 0))\n",
    "            self.mixer = Residual(SHSA(dim, qk_dim, pdim))\n",
    "            self.ffn = Residual(FFN(dim, int(dim * 2)))\n",
    "        elif type == \"i\":   # for early stages\n",
    "            self.conv = Residual(Conv2d_BN(dim, dim, 3, 1, 1, groups = dim, bn_weight_init = 0))\n",
    "            self.mixer = torch.nn.Identity()\n",
    "            self.ffn = Residual(FFN(dim, int(dim * 2)))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.ffn(self.mixer(self.conv(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimg_size = 512 # 256\\npatch_size=16\\nfrozen_stages = 0\\nin_chans = 3\\nembed_dim = [224, 336, 448]\\npartial_dim = [48, 72, 96] # partial_dim = r*embed_dim with r=1/4.67\\nqk_dim = [16, 16, 16]\\ndepth = [4, 7, 6]\\ntypes = [\"i\", \"s\", \"s\"]\\ndown_ops = [[\\'subsample\\', 2], [\\'subsample\\', 2], [\\'\\']]\\npretrained = None\\ndistillation = False\\n\\n\\ninput_image = torch.randn(1, 3, 512, 512)\\n\\npatch_embed = torch.nn.Sequential(Conv2d_BN(in_chans, embed_dim[0] // 8, 3, 2, 1), torch.nn.ReLU(),\\n                           Conv2d_BN(embed_dim[0] // 8, embed_dim[0] // 4, 3, 2, 1), torch.nn.ReLU(),\\n                           Conv2d_BN(embed_dim[0] // 4, embed_dim[0] // 2, 3, 2, 1), torch.nn.ReLU(),\\n                           Conv2d_BN(embed_dim[0] // 2, embed_dim[0], 3, 2, 1)\\n                           )\\n\\nx = patch_embed(input_image)\\nprint(\"patch_embed: \", x.shape)\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "img_size = 512 # 256\n",
    "patch_size=16\n",
    "frozen_stages = 0\n",
    "in_chans = 3\n",
    "embed_dim = [224, 336, 448]\n",
    "partial_dim = [48, 72, 96] # partial_dim = r*embed_dim with r=1/4.67\n",
    "qk_dim = [16, 16, 16]\n",
    "depth = [4, 7, 6]\n",
    "types = [\"i\", \"s\", \"s\"]\n",
    "down_ops = [['subsample', 2], ['subsample', 2], ['']]\n",
    "pretrained = None\n",
    "distillation = False\n",
    "\n",
    "\n",
    "input_image = torch.randn(1, 3, 512, 512)\n",
    "\n",
    "patch_embed = torch.nn.Sequential(Conv2d_BN(in_chans, embed_dim[0] // 8, 3, 2, 1), torch.nn.ReLU(),\n",
    "                           Conv2d_BN(embed_dim[0] // 8, embed_dim[0] // 4, 3, 2, 1), torch.nn.ReLU(),\n",
    "                           Conv2d_BN(embed_dim[0] // 4, embed_dim[0] // 2, 3, 2, 1), torch.nn.ReLU(),\n",
    "                           Conv2d_BN(embed_dim[0] // 2, embed_dim[0], 3, 2, 1)\n",
    "                           )\n",
    "\n",
    "x = patch_embed(input_image)\n",
    "print(\"patch_embed: \", x.shape)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegFormerHead(nn.Module):\n",
    "    def __init__(self, in_channels=448, num_classes=19):\n",
    "        \"\"\"\n",
    "        SegFormer Head for semantic segmentation.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels from the feature map.\n",
    "            num_classes (int): Number of segmentation classes.\n",
    "        \"\"\"\n",
    "        super(SegFormerHead, self).__init__()\n",
    "\n",
    "        # 1x1 convolution to reduce the number of channels to the number of classes\n",
    "        self.conv_1x1 = nn.Conv2d(in_channels, num_classes, kernel_size=1)\n",
    "\n",
    "        # Optional final upsampling to match input resolution\n",
    "        self.upsample = nn.Upsample(scale_factor=8, mode='bilinear', align_corners=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for SegFormer Head.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input feature map of shape (B, C, H, W).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Segmentation map of shape (B, num_classes, H*8, W*8).\n",
    "        \"\"\"\n",
    "        # Apply 1x1 convolution to reduce to num_classes\n",
    "        x = self.conv_1x1(x)\n",
    "\n",
    "        # Upsample the output to the original input resolution\n",
    "        x = self.upsample(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "    # Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Assuming input feature map has shape [1, 448, 8, 8]\n",
    "    feature_map = torch.randn(1, 448, 8, 8)\n",
    "    \n",
    "    # Instantiate the head with 19 classes (e.g., for ADE20K dataset)\n",
    "    segformer_head = SegFormerHead(in_channels=448, num_classes=19)\n",
    "    \n",
    "    # Forward pass\n",
    "    segmentation_map = segformer_head(feature_map)\n",
    "    print(\"Segmentation map shape:\", segmentation_map.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SHViT(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_chans=3,\n",
    "                 num_classes=1000,\n",
    "                 embed_dim=[128, 256, 384],\n",
    "                 partial_dim = [32, 64, 96],\n",
    "                 qk_dim=[16, 16, 16],\n",
    "                 depth=[1, 2, 3],\n",
    "                 types = [\"s\", \"s\", \"s\"],\n",
    "                 down_ops=[['subsample', 2], ['subsample', 2], ['']],\n",
    "                 distillation=False,):\n",
    "        super().__init__()\n",
    "\n",
    "        # Patch embedding\n",
    "        self.patch_embed = torch.nn.Sequential(Conv2d_BN(in_chans, embed_dim[0] // 8, 3, 2, 1), torch.nn.ReLU(),\n",
    "                           Conv2d_BN(embed_dim[0] // 8, embed_dim[0] // 4, 3, 2, 1), torch.nn.ReLU(),\n",
    "                           Conv2d_BN(embed_dim[0] // 4, embed_dim[0] // 2, 3, 2, 1), torch.nn.ReLU(),\n",
    "                           Conv2d_BN(embed_dim[0] // 2, embed_dim[0], 3, 2, 1))\n",
    "\n",
    "        self.blocks1 = []\n",
    "        self.blocks2 = []\n",
    "        self.blocks3 = []\n",
    "\n",
    "        # Build SHViT blocks\n",
    "        for i, (ed, kd, pd, dpth, do, t) in enumerate(\n",
    "                zip(embed_dim, qk_dim, partial_dim, depth, down_ops, types)):\n",
    "            for d in range(dpth):\n",
    "                eval('self.blocks' + str(i+1)).append(BasicBlock(ed, kd, pd, t))\n",
    "            if do[0] == 'subsample':\n",
    "                # Build SHViT downsample block\n",
    "                #('Subsample' stride)\n",
    "                blk = eval('self.blocks' + str(i+1))\n",
    "                blk.append(torch.nn.Sequential(Residual(Conv2d_BN(embed_dim[i], embed_dim[i], 3, 1, 1, groups=embed_dim[i])),\n",
    "                                    Residual(FFN(embed_dim[i], int(embed_dim[i] * 2))),))\n",
    "                blk.append(PatchMerging(*embed_dim[i:i + 2]))\n",
    "                \n",
    "                blk.append(torch.nn.Sequential(Residual(Conv2d_BN(embed_dim[i + 1], embed_dim[i + 1], 3, 1, 1, groups=embed_dim[i + 1])),\n",
    "                                    Residual(FFN(embed_dim[i + 1], int(embed_dim[i + 1] * 2))),))\n",
    "        self.blocks1 = torch.nn.Sequential(*self.blocks1)\n",
    "        self.blocks2 = torch.nn.Sequential(*self.blocks2)\n",
    "        self.blocks3 = torch.nn.Sequential(*self.blocks3)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        # Classification head\n",
    "        self.head = BN_Linear(embed_dim[-1], num_classes) if num_classes > 0 else torch.nn.Identity()\n",
    "        self.distillation = distillation\n",
    "        if distillation:\n",
    "            self.head_dist = BN_Linear(embed_dim[-1], num_classes) if num_classes > 0 else torch.nn.Identity()\n",
    "\n",
    "        \"\"\"    \n",
    "        \n",
    "        \n",
    "        # Semantic Segmentation\n",
    "        self.head = SegFormerHead()\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        x = self.blocks1(x)\n",
    "        x = self.blocks2(x)\n",
    "        x = self.blocks3(x)\n",
    "        x = torch.nn.functional.adaptive_avg_pool2d(x, 1).flatten(1)\n",
    "        if self.distillation:\n",
    "            x = self.head(x), self.head_dist(x)\n",
    "            if not self.training:\n",
    "                x = (x[0] + x[1]) / 2\n",
    "        else:\n",
    "            x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 256\n",
    "patch_size=16\n",
    "frozen_stages = 0\n",
    "in_chans = 3\n",
    "embed_dim = [224, 336, 448]\n",
    "partial_dim = [48, 72, 96] # partial_dim = r*embed_dim with r=1/4.67\n",
    "qk_dim = [16, 16, 16]\n",
    "depth = [4, 7, 6]\n",
    "types = [\"i\", \"s\", \"s\"]\n",
    "down_ops = [['subsample', 2], ['subsample', 2], ['']]\n",
    "pretrained = None\n",
    "distillation = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shvit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
